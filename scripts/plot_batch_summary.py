#!/usr/bin/env python3
"""
Plot batch latencies from batching_trace_summary.csv.

This script expects the CSV generated by batching tracing (when trace_enabled is
true). It produces three scatter plots:

1. Combined batches (CPU + GPU)
2. CPU-only batches
3. GPU-only batches

Usage:
  ./scripts/plot_batch_summary.py PATH_TO/batching_trace_summary.csv
  ./scripts/plot_batch_summary.py PATH_TO/file.csv --output plots.png
"""

from __future__ import annotations

import argparse
import csv
import json
import sys
from bisect import bisect_left
from collections import Counter, deque
from itertools import cycle
from pathlib import Path
from typing import Iterable, List, NamedTuple, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.gridspec import GridSpecFromSubplotSpec
from mpl_toolkits.axes_grid1.inset_locator import inset_axes

PHASE_LABELS = [
    "queue",
    "batch",
    "submit",
    "scheduling",
    "codelet",
    "inference",
    "callback",
]

PHASE_COLORS = {
    "queue": "#1f77b4",
    "batch": "#ff7f0e",
    "submit": "#2ca02c",
    "scheduling": "#d62728",
    "codelet": "#9467bd",
    "inference": "#8c564b",
    "callback": "#e377c2",
}

ROLLING_WINDOW = 50
THROUGHPUT_WINDOW = 100
MAX_BATCH_CDF_SERIES = 4
PHASE_INDEX = {label: idx for idx, label in enumerate(PHASE_LABELS)}
SLA_THRESHOLDS_MS = (50.0, 100.0, 200.0)
MAX_WORKER_CDFS = 6
CPU_COLOR = "#d62728"
GPU_COLOR = "#1f77b4"
BATCH_ID_LABEL = "Batch ID"
BATCH_SIZE_LABEL = "Batch size"
WORKER_ID_LABEL = "Worker ID"
LATENCY_MS_LABEL = "Latency (ms)"
TOTAL_TIME_MS_LABEL = "Total time (ms)"
AVERAGE_DURATION_MS_LABEL = "Average duration (ms)"
TIME_SINCE_FIRST_ARRIVAL_LABEL = "Time since first arrival (s)"
LEGEND_LOC_UPPER_RIGHT = "upper right"
LEGEND_LOC_LOWER_RIGHT = "lower right"
INSET_LOC_LOWER_LEFT = "lower left"

BatchRow = Tuple[
    int,  # batch_id
    float,  # total latency (ms)
    str,  # worker type
    int,  # worker_id
    int,  # batch size
    int,  # logical job count
    Tuple[float, ...],  # phase breakdown
    Tuple[int, ...],  # request arrival timestamps (us)
]


class PlotInputs(NamedTuple):
    all_ids: List[int]
    all_lat: List[float]
    all_workers: List[int]
    all_sizes: List[int]
    all_jobs: List[int]
    all_breakdowns: List[Tuple[float, ...]]
    all_arrivals: List[Tuple[int, ...]]
    cpu_ids: List[int]
    cpu_lat: List[float]
    cpu_workers: List[int]
    cpu_sizes: List[int]
    cpu_jobs: List[int]
    cpu_breakdowns: List[Tuple[float, ...]]
    cpu_arrivals: List[Tuple[int, ...]]
    gpu_ids: List[int]
    gpu_lat: List[float]
    gpu_workers: List[int]
    gpu_sizes: List[int]
    gpu_jobs: List[int]
    gpu_breakdowns: List[Tuple[float, ...]]
    gpu_arrivals: List[Tuple[int, ...]]
    all_worker_types: List[str]
    batch_times: List[float | None]
    summary_path: Path


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Plot batch latencies from batching_trace_summary.csv."
    )
    parser.add_argument(
        "summary_csv",
        type=Path,
        help="Path to batching_trace_summary.csv",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        help=(
            "Optional output image path (PNG/JPG/etc.). "
            "If omitted the plots are shown interactively."
        ),
    )
    return parser.parse_args()


def parse_request_arrival_field(field: str | None) -> Tuple[int, ...]:
    if not field:
        return ()
    values: List[int] = []
    for raw in field.split(";"):
        token = raw.strip()
        if not token:
            continue
        try:
            values.append(int(token))
        except ValueError as exc:
            raise ValueError(
                f"Invalid request_arrival_us entry '{raw}' in field '{field}'"
            ) from exc
    return tuple(values)


def load_latencies(
    csv_path: Path,
) -> List[BatchRow]:
    batches: List[BatchRow] = []
    with csv_path.open(newline="") as handle:
        reader = csv.DictReader(handle)
        required = {
            "batch_id",
            "total_ms",
            "worker_type",
            "worker_id",
            "batch_size",
        }
        required.update(f"{label}_ms" for label in PHASE_LABELS)
        missing = sorted(required - set(reader.fieldnames or []))
        if missing:
            raise ValueError(
                f"{csv_path} is missing required columns: {', '.join(missing)}"
            )
        for row in reader:
            try:
                batch_id = int(row["batch_id"])
                latency = float(row["total_ms"])
            except ValueError as exc:
                raise ValueError(f"Invalid numeric values in row: {row}") from exc
            worker_type = (row.get("worker_type") or "unknown").strip().lower()
            worker_id = int(row["worker_id"])
            batch_size = int(row["batch_size"])
            logical_jobs_field = row.get("logical_jobs")
            logical_jobs = (
                int(logical_jobs_field)
                if logical_jobs_field not in (None, "")
                else batch_size
            )
            breakdown = tuple(float(row[f"{label}_ms"]) for label in PHASE_LABELS)
            arrivals_field = row.get("request_arrival_us")
            arrivals = parse_request_arrival_field(arrivals_field)
            batches.append(
                (
                    batch_id,
                    latency,
                    worker_type,
                    worker_id,
                    batch_size,
                    logical_jobs,
                    breakdown,
                    arrivals,
                )
            )
    return batches


def _compute_batch_times(arrivals: Sequence[Sequence[int]]) -> List[float | None]:
    times: List[float | None] = []
    for sequence in arrivals:
        if not sequence:
            times.append(None)
            continue
        valid = [value for value in sequence if value > 0]
        if not valid:
            times.append(None)
            continue
        times.append(float(min(valid)))
    if not times:
        return []
    non_null = [t for t in times if t is not None]
    if not non_null:
        return times
    origin = min(non_null)
    return [t - origin if t is not None else None for t in times]


def filter_latencies(
    data: Iterable[BatchRow],
    *,
    worker_type: str | None = None,
) -> Tuple[
    List[int],
    List[float],
    List[int],
    List[int],
    List[int],
    List[Tuple[float, ...]],
    List[Tuple[int, ...]],
]:
    ids: List[int] = []
    latencies: List[float] = []
    worker_ids: List[int] = []
    batch_sizes: List[int] = []
    logical_jobs: List[int] = []
    breakdowns: List[Tuple[float, ...]] = []
    arrivals: List[Tuple[int, ...]] = []
    for (
        batch_id,
        latency,
        device,
        worker_id,
        batch_size,
        logical_job_count,
        breakdown,
        request_arrivals,
    ) in data:
        if worker_type is None or device == worker_type:
            ids.append(batch_id)
            latencies.append(latency)
            worker_ids.append(worker_id)
            batch_sizes.append(batch_size)
            logical_jobs.append(logical_job_count)
            breakdowns.append(breakdown)
            arrivals.append(request_arrivals)
    return ids, latencies, worker_ids, batch_sizes, logical_jobs, breakdowns, arrivals


def flatten_request_arrival_seconds(
    arrival_sequences: Sequence[Sequence[int]],
) -> List[float]:
    events: List[int] = []
    for sequence in arrival_sequences:
        for timestamp in sequence:
            if timestamp > 0:
                events.append(timestamp)
    if not events:
        return []
    events.sort()
    start = events[0]
    relative_seconds = [
        (timestamp - start) / 1_000_000.0 for timestamp in events if timestamp >= start
    ]
    return relative_seconds


def plot_latency_stack(
    ax,
    batch_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    if not batch_ids or not breakdowns:
        ax.set_title("Latency composition per batch (no data)")
        ax.set_xlabel(BATCH_ID_LABEL)
        ax.set_ylabel("Latency contribution (ms)")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    labels = PHASE_LABELS
    components = list(map(list, zip(*breakdowns)))
    bottom = [0.0] * len(batch_ids)
    ax.set_title("Latency composition per batch")
    for values, label in zip(components, labels):
        ax.bar(batch_ids, values, bottom=bottom, label=label, width=0.6)
        bottom = [b + v for b, v in zip(bottom, values)]
    ax.set_xlabel(BATCH_ID_LABEL)
    ax.set_ylabel("Latency contribution (ms)")
    ax.legend(loc=LEGEND_LOC_UPPER_RIGHT, fontsize="small")
    ax.grid(True, linestyle="--", alpha=0.3)


def scatter_plot(
    ax, x: List[int], y: List[float], title: str, *, color: str | None = None
) -> None:
    if not x:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel(BATCH_ID_LABEL)
        ax.set_ylabel(LATENCY_MS_LABEL)
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    ax.scatter(x, y, s=14, alpha=0.7, c=color)
    ax.set_title(title)
    ax.set_xlabel(BATCH_ID_LABEL)
    ax.set_ylabel(LATENCY_MS_LABEL)
    ax.grid(True, linestyle="--", alpha=0.4)


def plot_violin(
    ax,
    data: Sequence[float],
    label: str,
    color: str | None = None,
) -> None:
    if not data:
        ax.set_title(f"{label} (no data)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    parts = ax.violinplot(data, showmeans=True, showmedians=False)
    if color:
        for pc in parts["bodies"]:
            pc.set_facecolor(color)
            pc.set_edgecolor("black")
        parts["cbars"].set_edgecolor(color)
        parts["cmins"].set_edgecolor(color)
        parts["cmaxes"].set_edgecolor(color)
    ax.set_title(label)
    ax.grid(True, linestyle="--", alpha=0.4)


def _normalize_marker_sizes(sizes: Sequence[int]) -> List[float]:
    if not sizes:
        return []
    max_size = max(sizes)
    if max_size <= 0:
        return [50.0] * len(sizes)
    base = 50.0
    span = 200.0
    return [base + (value / max_size) * span for value in sizes]


def _bucket_points_by_worker(
    x: Sequence[int],
    y: Sequence[float],
    worker_types: Sequence[str],
) -> dict[str, tuple[List[int], List[float]]]:
    buckets = {
        "gpu": ([], []),
        "cpu": ([], []),
        "other": ([], []),
    }
    for px, py, worker in zip(x, y, worker_types):
        kind = (worker or "").lower()
        if kind == "cpu":
            bucket = "cpu"
        elif kind in ("cuda", "gpu"):
            bucket = "gpu"
        else:
            bucket = "other"
        buckets[bucket][0].append(px)
        buckets[bucket][1].append(py)
    return buckets


def _overlay_worker_types(
    ax, x: Sequence[int], y: Sequence[float], worker_types: Sequence[str] | None
) -> None:
    if not worker_types or len(worker_types) != len(x):
        return
    buckets = _bucket_points_by_worker(x, y, worker_types)
    overlay_kwargs = {"s": 5, "alpha": 0.95, "zorder": 3, "marker": "o"}
    handles = []
    for label, key, color in (("GPU", "gpu", "#1f77b4"), ("CPU", "cpu", "#d62728")):
        xs, ys = buckets[key]
        if xs:
            handles.append(
                ax.scatter(xs, ys, color=color, label=label, **overlay_kwargs)
            )
    other_xs, other_ys = buckets["other"]
    if other_xs:
        handles.append(
            ax.scatter(
                other_xs, other_ys, color="#7f7f7f", label="Other", **overlay_kwargs
            )
        )
    if handles:
        ax.legend(handles=handles, loc="upper left", fontsize="small")


def _find_trace_path(summary_path: Path) -> Path | None:
    stem = summary_path.stem
    base = stem
    for suffix in ("_probe_summary", "_summary"):
        if stem.endswith(suffix):
            base = stem[: -len(suffix)]
            break
    candidate = summary_path.with_name(f"{base}.json")
    if candidate.exists():
        return candidate
    return None


def _load_congestion_spans(summary_path: Path) -> list[dict[str, float]]:
    trace_path = _find_trace_path(summary_path)
    if trace_path is None:
        return []
    spans: list[dict[str, float]] = []
    try:
        with trace_path.open() as handle:
            for raw_line in handle:
                line = raw_line.strip()
                if '"congestion"' not in line:
                    continue
                try:
                    event = json.loads(line.rstrip(","))
                except json.JSONDecodeError:
                    continue
                if event.get("name") != "congestion":
                    continue
                ts = float(event.get("ts", 0.0))
                duration = float(event.get("dur", 0.0))
                args = event.get("args", {}) or {}
                spans.append(
                    {
                        "start": ts,
                        "end": ts + duration,
                        "enter": float(args.get("enter_threshold", 0.0)),
                        "clear": float(args.get("clear_threshold", 0.0)),
                    }
                )
    except OSError:
        return []
    return spans


def _locate_id_for_time(
    sorted_times: Sequence[float], sorted_ids: Sequence[int], target: float
) -> int:
    if not sorted_times or not sorted_ids:
        return 0
    pos = bisect_left(sorted_times, target)
    if pos <= 0:
        return sorted_ids[0]
    if pos >= len(sorted_times):
        return sorted_ids[-1]
    before_diff = abs(sorted_times[pos - 1] - target)
    after_diff = abs(sorted_times[pos] - target)
    return sorted_ids[pos] if after_diff < before_diff else sorted_ids[pos - 1]


def _overlay_congestion_zones(
    ax: plt.Axes,
    batch_ids: Sequence[int],
    batch_times: Sequence[float | None],
    summary_path: Path,
) -> None:
    spans = _load_congestion_spans(summary_path)
    if not spans or not batch_ids or not batch_times:
        return
    pairs = [
        (time, bid) for time, bid in zip(batch_times, batch_ids) if time is not None
    ]
    if not pairs:
        return
    pairs.sort(key=lambda pair: pair[0])
    times = [pair[0] for pair in pairs]
    ids = [pair[1] for pair in pairs]
    min_span_start = min(span["start"] for span in spans)
    time_offset = times[0] - min_span_start
    label_added = False
    for span in spans:
        start_time = span["start"] + time_offset
        end_time = span["end"] + time_offset
        start_id = _locate_id_for_time(times, ids, start_time)
        end_id = _locate_id_for_time(times, ids, end_time)
        x0, x1 = sorted((start_id, end_id))
        ax.axvspan(
            x0,
            x1,
            color="#ffe6e6",
            alpha=0.35,
            label="Congestion" if not label_added else None,
        )
        label_added = True
    if label_added:
        ax.legend(loc="upper left", fontsize="small")


def scatter_with_size(
    ax,
    x: Sequence[int],
    y: Sequence[float],
    sizes: Sequence[int],
    title: str,
    worker_types: Sequence[str] | None = None,
) -> None:
    if not x or not y or not sizes:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel(BATCH_ID_LABEL)
        ax.set_ylabel(LATENCY_MS_LABEL)
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    scale = _normalize_marker_sizes(sizes)
    scatter = ax.scatter(
        x,
        y,
        s=scale,
        c=sizes,
        cmap="viridis",
        alpha=0.7,
    )
    _overlay_worker_types(ax, x, y, worker_types)
    ax.set_title(title)
    ax.set_xlabel(BATCH_ID_LABEL)
    ax.set_ylabel(LATENCY_MS_LABEL)
    ax.grid(True, linestyle="--", alpha=0.4)
    # Place colorbar outside the axes to avoid shrinking or covering data.
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc=INSET_LOC_LOWER_LEFT,
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(scatter, cax=cax)
    cbar.set_label(BATCH_SIZE_LABEL)


def plot_worker_boxplots(
    ax,
    worker_ids: Sequence[int],
    latencies: Sequence[float],
) -> None:
    data = {}
    for worker, latency in zip(worker_ids, latencies):
        if worker < 0:
            continue
        data.setdefault(worker, []).append(latency)
    if not data:
        ax.set_title("Worker latency boxplots (no data)")
        ax.set_xlabel(WORKER_ID_LABEL)
        ax.set_ylabel(LATENCY_MS_LABEL)
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    sorted_items = sorted(data.items(), key=lambda item: item[0])
    ax.boxplot(
        [item[1] for item in sorted_items],
        labels=[item[0] for item in sorted_items],
        showmeans=True,
    )
    ax.set_title("Worker latency boxplots")
    ax.set_xlabel(WORKER_ID_LABEL)
    ax.set_ylabel(LATENCY_MS_LABEL)
    ax.grid(True, linestyle="--", alpha=0.4)


def plot_worker_radar(
    ax,
    worker_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_workers: int = 4,
) -> None:
    contributions: dict[int, np.ndarray] = {}
    counts: dict[int, int] = {}
    for worker, phases in zip(worker_ids, breakdowns):
        if worker < 0:
            continue
        contributions.setdefault(worker, np.zeros(len(PHASE_LABELS)))
        contributions[worker] += np.array(phases)
        counts[worker] = counts.get(worker, 0) + 1

    if not contributions:
        ax.set_title("Worker radar (no data)")
        ax.set_axis_off()
        return

    averages = []
    for worker, total in contributions.items():
        average = total / counts[worker]
        averages.append((worker, average))
    averages.sort(key=lambda item: counts[item[0]], reverse=True)
    averages = averages[:max_workers]

    angles = np.linspace(0, 2 * np.pi, len(PHASE_LABELS), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))
    color_cycle = cycle(["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728"])
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_xticks(angles[:-1], PHASE_LABELS)
    ax.set_title("Worker radar (avg phase ms)")

    for worker, avg in averages:
        values = np.concatenate((avg, [avg[0]]))
        color = next(color_cycle)
        ax.plot(angles, values, label=f"worker {worker}", color=color)
        ax.fill(angles, values, alpha=0.1, color=color)

    ax.legend(loc=LEGEND_LOC_UPPER_RIGHT, bbox_to_anchor=(1.2, 1.1), fontsize="small")


def plot_worker_phase_utilization(
    ax,
    worker_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    contributions: dict[int, np.ndarray] = {}
    for worker, phases in zip(worker_ids, breakdowns):
        if worker < 0:
            continue
        contributions.setdefault(worker, np.zeros(len(PHASE_LABELS)))
        contributions[worker] += np.array(phases)
    if not contributions:
        ax.set_title("Worker phase utilization (no data)")
        ax.set_xlabel(TOTAL_TIME_MS_LABEL)
        ax.set_ylabel(WORKER_ID_LABEL)
        ax.grid(True, axis="x", linestyle="--", alpha=0.3)
        return
    sorted_items = sorted(
        contributions.items(), key=lambda item: item[1].sum(), reverse=True
    )
    worker_labels = [str(worker) for worker, _ in sorted_items]
    y_pos = np.arange(len(sorted_items))
    left = np.zeros(len(sorted_items), dtype=float)
    legend_added: set[str] = set()
    for idx, phase in enumerate(PHASE_LABELS):
        values = [item[1][idx] for item in sorted_items]
        color = PHASE_COLORS.get(phase, None)
        ax.barh(
            y_pos,
            values,
            left=left,
            color=color,
            label=phase if phase not in legend_added else None,
        )
        left += np.array(values)
        legend_added.add(phase)
    ax.set_yticks(y_pos, worker_labels)
    ax.set_xlabel(TOTAL_TIME_MS_LABEL)
    ax.set_ylabel(WORKER_ID_LABEL)
    ax.set_title("Worker phase utilization")
    ax.grid(True, axis="x", linestyle="--", alpha=0.3)
    ax.legend(loc=LEGEND_LOC_UPPER_RIGHT, fontsize="small")


def plot_worker_time_heatmap(
    ax,
    worker_ids: Sequence[int],
    batch_ids: Sequence[int],
    latencies: Sequence[float],
    max_workers: int = 12,
    bucket_size: int = 100,
) -> None:
    if not worker_ids or not batch_ids or not latencies:
        ax.set_title("Worker-time heatmap (no data)")
        ax.set_xlabel("Batch bucket")
        ax.set_ylabel(WORKER_ID_LABEL)
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    sorted_workers = sorted(set(worker_ids), key=lambda wid: (wid < 0, wid))[
        :max_workers
    ]
    worker_index = {wid: idx for idx, wid in enumerate(sorted_workers)}
    bucket_map: dict[tuple[int, int], list[float]] = {}
    for wid, bid, latency in zip(worker_ids, batch_ids, latencies):
        if wid not in worker_index or wid < 0:
            continue
        bucket = bid // bucket_size
        bucket_map.setdefault((wid, bucket), []).append(latency)

    if not bucket_map:
        ax.set_title("Worker-time heatmap (no data)")
        ax.set_xlabel("Batch bucket")
        ax.set_ylabel(WORKER_ID_LABEL)
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    buckets = sorted({bucket for _, bucket in bucket_map.keys()})
    heatmap = np.full((len(sorted_workers), len(buckets)), np.nan)
    bucket_index = {bucket: idx for idx, bucket in enumerate(buckets)}
    for (wid, bucket), samples in bucket_map.items():
        row = worker_index[wid]
        col = bucket_index[bucket]
        heatmap[row, col] = np.mean(samples)

    im = ax.imshow(heatmap, aspect="auto", cmap="plasma", origin="lower")
    ax.set_xticks(
        range(len(buckets)), [str(bucket * bucket_size) for bucket in buckets]
    )
    ax.set_yticks(range(len(sorted_workers)), [str(w) for w in sorted_workers])
    ax.set_xlabel("Batch ID bucket")
    ax.set_ylabel(WORKER_ID_LABEL)
    ax.set_title("Worker-time heatmap (avg latency)")
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc=INSET_LOC_LOWER_LEFT,
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label(LATENCY_MS_LABEL)


def plot_phase_heatmap(
    ax,
    batch_sizes: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_columns: int = 12,
) -> None:
    if not batch_sizes or not breakdowns:
        ax.set_title("Phase heatmap (no data)")
        ax.set_xlabel(BATCH_SIZE_LABEL)
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return

    counter = Counter(batch_sizes)
    top_sizes = [size for size, _ in counter.most_common(max_columns)]
    top_sizes.sort()
    if not top_sizes:
        ax.set_title("Phase heatmap (no data)")
        ax.set_xlabel(BATCH_SIZE_LABEL)
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return

    index = {size: idx for idx, size in enumerate(top_sizes)}
    totals = np.zeros((len(PHASE_LABELS), len(top_sizes)), dtype=float)
    counts = np.zeros_like(totals)

    for size, phases in zip(batch_sizes, breakdowns):
        col = index.get(size)
        if col is None:
            continue
        for row, value in enumerate(phases):
            totals[row, col] += value
            counts[row, col] += 1

    with np.errstate(invalid="ignore"):
        averages = np.divide(totals, counts, where=counts > 0)

    masked = np.ma.masked_invalid(averages)
    im = ax.imshow(masked, aspect="auto", cmap="magma", origin="lower")
    ax.set_xticks(range(len(top_sizes)), [str(size) for size in top_sizes])
    ax.set_yticks(range(len(PHASE_LABELS)), PHASE_LABELS)
    ax.set_xlabel(BATCH_SIZE_LABEL)
    ax.set_ylabel("Phase")
    ax.set_title("Phase heatmap (avg ms) - All workers")
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc=INSET_LOC_LOWER_LEFT,
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label(AVERAGE_DURATION_MS_LABEL)


def plot_worker_phase_heatmap(
    ax,
    worker_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_workers: int = 12,
) -> None:
    totals: dict[int, np.ndarray] = {}
    counts: dict[int, int] = {}
    for worker, phases in zip(worker_ids, breakdowns):
        if worker < 0:
            continue
        totals.setdefault(worker, np.zeros(len(PHASE_LABELS), dtype=float))
        totals[worker] += np.array(phases)
        counts[worker] = counts.get(worker, 0) + 1
    if not totals:
        ax.set_title("Phase heatmap by worker (no data)")
        ax.set_xlabel(WORKER_ID_LABEL)
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    sorted_workers = sorted(
        totals.keys(), key=lambda worker: (counts.get(worker, 0), worker), reverse=True
    )[:max_workers]
    matrix = np.zeros((len(PHASE_LABELS), len(sorted_workers)), dtype=float)
    for col, worker in enumerate(sorted_workers):
        average = totals[worker] / max(1, counts.get(worker, 1))
        matrix[:, col] = average
    im = ax.imshow(matrix, aspect="auto", cmap="magma", origin="lower")
    ax.set_xticks(
        range(len(sorted_workers)), [str(worker) for worker in sorted_workers]
    )
    ax.set_yticks(range(len(PHASE_LABELS)), PHASE_LABELS)
    ax.set_xlabel(WORKER_ID_LABEL)
    ax.set_ylabel("Phase")
    ax.set_title("Phase heatmap by worker (avg ms)")
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc=INSET_LOC_LOWER_LEFT,
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label(AVERAGE_DURATION_MS_LABEL)


def plot_phase_correlation(
    ax,
    breakdowns: Sequence[Tuple[float, ...]],
    *,
    title: str | None = None,
) -> None:
    base_title = title or "Phase correlation heatmap"
    if not breakdowns:
        ax.set_title(f"{base_title} (no data)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    matrix = np.array(breakdowns, dtype=float)
    if matrix.ndim != 2 or matrix.shape[0] < 2:
        ax.set_title(f"{base_title} (insufficient data)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    with np.errstate(invalid="ignore"):
        corr = np.corrcoef(matrix, rowvar=False)
    if np.all(np.isnan(corr)):
        ax.set_title(f"{base_title} (no variation)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    masked = np.ma.masked_invalid(corr)
    im = ax.imshow(
        masked,
        cmap="coolwarm",
        origin="lower",
        vmin=-1,
        vmax=1,
        aspect="auto",
    )
    ax.set_xticks(range(len(PHASE_LABELS)), PHASE_LABELS, rotation=45, ha="right")
    ax.set_yticks(range(len(PHASE_LABELS)), PHASE_LABELS)
    ax.set_xlabel("Phase")
    ax.set_ylabel("Phase")
    ax.set_title(base_title)
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc=INSET_LOC_LOWER_LEFT,
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label("Pearson r")


def plot_phase_pareto(
    ax,
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    if not breakdowns:
        ax.set_title("Phase Pareto (no data)")
        ax.set_xlabel("Phase")
        ax.set_ylabel(AVERAGE_DURATION_MS_LABEL)
        ax.grid(True, axis="y", linestyle="--", alpha=0.3)
        return
    totals = np.zeros(len(PHASE_LABELS), dtype=float)
    for phases in breakdowns:
        totals += np.array(phases)
    averages = totals / len(breakdowns)
    sorted_indices = np.argsort(-averages)
    sorted_labels = [PHASE_LABELS[idx] for idx in sorted_indices]
    sorted_values = averages[sorted_indices]
    ax.bar(sorted_labels, sorted_values, color="#6a4c93")
    ax.set_title("Phase Pareto (avg ms)")
    ax.set_xlabel("Phase")
    ax.set_ylabel(AVERAGE_DURATION_MS_LABEL)
    ax.grid(True, axis="y", linestyle="--", alpha=0.3)


def plot_phase_waterfall(
    ax,
    cpu_breakdowns: Sequence[Tuple[float, ...]],
    gpu_breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    categories = []
    data = []
    if cpu_breakdowns:
        categories.append("CPU")
        data.append(np.array(cpu_breakdowns))
    if gpu_breakdowns:
        categories.append("GPU")
        data.append(np.array(gpu_breakdowns))
    if not categories:
        ax.set_title("Phase waterfall (no data)")
        ax.set_xlabel(TOTAL_TIME_MS_LABEL)
        ax.set_ylabel("Worker type")
        ax.grid(True, axis="x", linestyle="--", alpha=0.3)
        return

    y_positions = np.arange(len(categories))
    legend_added = set()
    for idx, (label, breakdown_array) in enumerate(zip(categories, data)):
        totals = breakdown_array.sum(axis=0)
        left = 0.0
        for phase, value in zip(PHASE_LABELS, totals):
            color = PHASE_COLORS.get(phase, None)
            ax.barh(
                y_positions[idx],
                value,
                left=left,
                color=color,
                label=phase if phase not in legend_added else None,
            )
            left += value
            legend_added.add(phase)
    ax.set_yticks(y_positions, categories)
    ax.set_xlabel(TOTAL_TIME_MS_LABEL)
    ax.set_title("Phase waterfall (CPU vs GPU)")
    ax.grid(True, axis="x", linestyle="--", alpha=0.3)
    ax.legend(loc=LEGEND_LOC_UPPER_RIGHT, fontsize="small")


def compute_moving_average(
    ids: Sequence[int], values: Sequence[float], window: int = 50
) -> Tuple[List[int], List[float]]:
    if not ids or not values:
        return [], []
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    sorted_ids = [pair[0] for pair in sorted_pairs]
    sorted_vals = [pair[1] for pair in sorted_pairs]
    window = max(1, min(window, len(sorted_vals)))
    rolling_ids: List[int] = []
    rolling_vals: List[float] = []
    window_sum = 0.0
    current = deque()
    for batch_id, value in zip(sorted_ids, sorted_vals):
        current.append(value)
        window_sum += value
        if len(current) > window:
            window_sum -= current.popleft()
        rolling_ids.append(batch_id)
        rolling_vals.append(window_sum / len(current))
    return rolling_ids, rolling_vals


def compute_rolling_percentiles(
    ids: Sequence[int],
    values: Sequence[float],
    *,
    window_size: int = 50,
    percentiles: Sequence[int] = (50, 95, 99),
) -> dict[int, Tuple[List[int], List[float]]]:
    percentiles = tuple(percentiles)
    result = {perc: ([], []) for perc in percentiles}
    if not ids or not values:
        return result
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    window = deque()
    window_size = max(1, window_size)
    for batch_id, value in sorted_pairs:
        window.append(float(value))
        if len(window) > window_size:
            window.popleft()
        window_array = np.fromiter(window, dtype=float)
        percentile_values = np.percentile(window_array, percentiles)
        for perc, perc_value in zip(percentiles, percentile_values):
            result[perc][0].append(batch_id)
            result[perc][1].append(float(perc_value))
    return result


def compute_cumulative_latency(
    ids: Sequence[int], values: Sequence[float]
) -> Tuple[List[int], List[float]]:
    if not ids or not values:
        return [], []
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    cum_sum = 0.0
    cum_ids: List[int] = []
    cum_vals: List[float] = []
    for batch_id, latency in sorted_pairs:
        cum_sum += latency
        cum_ids.append(batch_id)
        cum_vals.append(cum_sum)
    return cum_ids, cum_vals


def compute_throughput(
    ids: Sequence[int],
    jobs: Sequence[int],
    window_size: int = THROUGHPUT_WINDOW,
) -> Tuple[List[int], List[float]]:
    if not ids or not jobs:
        return [], []
    sorted_pairs = sorted(zip(ids, jobs), key=lambda pair: pair[0])
    throughput_ids: List[int] = []
    throughput_vals: List[float] = []
    window = deque()
    jobs_sum = 0
    batch_sum = 0
    for batch_id, job_count in sorted_pairs:
        window.append((batch_id, job_count))
        jobs_sum += job_count
        batch_sum += 1
        while window and batch_sum > window_size:
            _, oldest_jobs = window.popleft()
            jobs_sum -= oldest_jobs
            batch_sum -= 1
        throughput_ids.append(batch_id)
        throughput_vals.append(jobs_sum / max(1, batch_sum))
    return throughput_ids, throughput_vals


def plot_request_arrival_timeline(
    ax,
    arrival_sequences: Sequence[Sequence[int]],
) -> None:
    relative_seconds = flatten_request_arrival_seconds(arrival_sequences)
    if not relative_seconds:
        ax.set_title("Request arrival timeline (no data)")
        ax.set_xlabel(TIME_SINCE_FIRST_ARRIVAL_LABEL)
        ax.set_ylabel("Cumulative requests")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    counts = np.arange(1, len(relative_seconds) + 1)
    ax.step(relative_seconds, counts, where="post", color="#ff9896")
    ax.set_title("Request arrival timeline")
    ax.set_xlabel(TIME_SINCE_FIRST_ARRIVAL_LABEL)
    ax.set_ylabel("Cumulative requests")
    ax.grid(True, linestyle="--", alpha=0.3)
    if relative_seconds:
        duration = relative_seconds[-1]
        if duration > 0:
            avg_rate = counts[-1] / duration
            ax.text(
                0.98,
                0.05,
                f"avg {avg_rate:.1f} req/s",
                transform=ax.transAxes,
                ha="right",
                va="bottom",
                fontsize="small",
                color="#444444",
            )


def plot_request_arrival_rate(
    ax,
    arrival_sequences: Sequence[Sequence[int]],
    *,
    bin_width: float = 1.0,
    max_bins: int = 200,
) -> None:
    relative_seconds = flatten_request_arrival_seconds(arrival_sequences)
    if not relative_seconds:
        ax.set_title("Request arrival rate (no data)")
        ax.set_xlabel(TIME_SINCE_FIRST_ARRIVAL_LABEL)
        ax.set_ylabel("Requests/s")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    duration = max(relative_seconds[-1], bin_width)
    bin_width = max(bin_width, 0.1)
    bin_count = min(max_bins, max(1, int(np.ceil(duration / bin_width))))
    bins = np.linspace(0.0, bin_count * bin_width, num=bin_count + 1)
    counts, edges = np.histogram(relative_seconds, bins=bins)
    rates = counts / bin_width
    ax.bar(
        edges[:-1],
        rates,
        width=bin_width * 0.9,
        align="edge",
        color="#98df8a",
    )
    ax.set_title(f"Request arrival rate (bin={bin_width:.1f}s)")
    ax.set_xlabel(TIME_SINCE_FIRST_ARRIVAL_LABEL)
    ax.set_ylabel("Requests/s")
    ax.grid(True, linestyle="--", alpha=0.3)
    peak_rate = rates.max() if len(rates) else 0.0
    ax.text(
        0.98,
        0.85,
        f"peak {peak_rate:.1f} req/s",
        transform=ax.transAxes,
        ha="right",
        va="top",
        fontsize="small",
        color="#444444",
    )


def compute_sla_coverage(
    ids: Sequence[int],
    latencies: Sequence[float],
    thresholds: Sequence[float] = SLA_THRESHOLDS_MS,
) -> dict[float, Tuple[List[int], List[float]]]:
    result = {thr: ([], []) for thr in thresholds}
    if not ids or not latencies:
        return result
    sorted_pairs = sorted(zip(ids, latencies), key=lambda pair: pair[0])
    total = 0
    passed = dict.fromkeys(thresholds, 0)
    for batch_id, latency in sorted_pairs:
        total += 1
        for thr in thresholds:
            if latency <= thr:
                passed[thr] += 1
            coverage = (passed[thr] / total) * 100.0
            result[thr][0].append(batch_id)
            result[thr][1].append(coverage)
    return result


def plot_sla_coverage(ax, ids: Sequence[int], latencies: Sequence[float]) -> None:
    series = compute_sla_coverage(ids, latencies)
    title = "SLA coverage (cumulative % <= threshold)"
    has_data = any(series[thr][0] for thr in SLA_THRESHOLDS_MS)
    if not has_data:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel(BATCH_ID_LABEL)
        ax.set_ylabel("Coverage (%)")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    colors = ["#1f77b4", "#ff7f0e", "#2ca02c"]
    for idx, thr in enumerate(SLA_THRESHOLDS_MS):
        ids_series, coverage = series[thr]
        label = f"<= {thr:.0f} ms"
        ax.plot(ids_series, coverage, label=label, color=colors[idx % len(colors)])
    ax.set_title(title)
    ax.set_xlabel(BATCH_ID_LABEL)
    ax.set_ylabel("Coverage (%)")
    ax.set_ylim(0, 105)
    ax.grid(True, linestyle="--", alpha=0.3)
    ax.legend(loc=LEGEND_LOC_LOWER_RIGHT, fontsize="small")


def compute_empirical_cdf(values: Sequence[float]) -> Tuple[List[float], List[float]]:
    if not values:
        return [], []
    arr = np.sort(np.array(values, dtype=float))
    probabilities = np.arange(1, len(arr) + 1, dtype=float) / len(arr)
    return arr.tolist(), probabilities.tolist()


def plot_latency_cdf(
    ax,
    datasets: Sequence[Sequence[float]],
    labels: Sequence[str],
    title: str,
) -> None:
    colors = [
        "#1f77b4",
        "#ff7f0e",
        "#2ca02c",
        "#d62728",
        "#9467bd",
        "#17becf",
    ]
    plotted = False
    for idx, (series, label) in enumerate(zip(datasets, labels)):
        if not series:
            continue
        xs, ys = compute_empirical_cdf(series)
        ax.step(
            xs,
            ys,
            where="post",
            label=label,
            color=colors[idx % len(colors)],
        )
        plotted = True
    if not plotted:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel(LATENCY_MS_LABEL)
        ax.set_ylabel("Cumulative probability")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    ax.set_title(title)
    ax.set_xlabel(LATENCY_MS_LABEL)
    ax.set_ylabel("Cumulative probability")
    ax.set_ylim(0, 1.01)
    ax.grid(True, linestyle="--", alpha=0.3)
    ax.legend(loc=LEGEND_LOC_LOWER_RIGHT, fontsize="small")


def plot_worker_cdf_grid(
    ax,
    worker_ids: Sequence[int],
    latencies: Sequence[float],
    max_workers: int = MAX_WORKER_CDFS,
) -> None:
    buckets: dict[int, List[float]] = {}
    for worker, latency in zip(worker_ids, latencies):
        if worker < 0:
            continue
        buckets.setdefault(worker, []).append(latency)
    if not buckets:
        ax.set_title("Worker latency CDFs (no data)")
        ax.set_axis_off()
        return
    top_workers = sorted(buckets.items(), key=lambda item: len(item[1]), reverse=True)[
        :max_workers
    ]
    cols = int(np.ceil(np.sqrt(len(top_workers))))
    rows = int(np.ceil(len(top_workers) / cols))
    fig = ax.figure
    subspec = ax.get_subplotspec()
    ax.remove()
    grid = GridSpecFromSubplotSpec(
        rows,
        cols,
        subplot_spec=subspec,
        wspace=0.25,
        hspace=0.25,
    )
    bbox = subspec.get_position(fig)
    for idx, (worker, samples) in enumerate(top_workers):
        row = idx // cols
        col = idx % cols
        cell = fig.add_subplot(grid[row, col])
        xs, ys = compute_empirical_cdf(samples)
        cell.step(xs, ys, where="post", color="#1f77b4")
        cell.set_title(f"worker {worker}")
        if row == rows - 1:
            cell.set_xlabel(LATENCY_MS_LABEL)
        else:
            cell.set_xticklabels([])
        if col == 0:
            cell.set_ylabel("CDF")
        else:
            cell.set_yticklabels([])
        cell.set_ylim(0, 1.01)
        cell.grid(True, linestyle="--", alpha=0.2)
    remaining = len(buckets) - len(top_workers)
    if remaining > 0:
        fig.text(
            bbox.x0 + bbox.width,
            bbox.y0 + bbox.height,
            f"+{remaining} more",
            fontsize="small",
            ha="right",
            va="bottom",
        )


def plot_rolling_percentiles(
    ax,
    batch_ids: Sequence[int],
    latencies: Sequence[float],
    worker_label: str,
    *,
    window_size: int = ROLLING_WINDOW,
) -> None:
    percentiles = (50, 95, 99)
    series = compute_rolling_percentiles(
        batch_ids,
        latencies,
        window_size=window_size,
        percentiles=percentiles,
    )
    has_data = any(series[perc][0] for perc in percentiles)
    title = f"{worker_label} rolling latency percentiles (window={window_size})"
    if not has_data:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel(BATCH_ID_LABEL)
        ax.set_ylabel(LATENCY_MS_LABEL)
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    colors = {50: "#1f77b4", 95: "#ff7f0e", 99: "#d62728"}
    for perc in percentiles:
        ids_series, values_series = series[perc]
        ax.plot(ids_series, values_series, label=f"P{perc}", color=colors.get(perc))
    ax.set_title(title)
    ax.set_xlabel(BATCH_ID_LABEL)
    ax.set_ylabel(LATENCY_MS_LABEL)
    ax.grid(True, linestyle="--", alpha=0.3)
    ax.legend(loc=LEGEND_LOC_UPPER_RIGHT, fontsize="small")


def plot_worker_task_distribution(
    ax,
    worker_ids: Sequence[int],
    jobs: Sequence[int],
) -> None:
    totals: dict[int, int] = {}
    for worker, job_count in zip(worker_ids, jobs):
        if worker < 0:
            continue
        totals[worker] = totals.get(worker, 0) + int(job_count)
    if not totals:
        ax.set_title("Worker task distribution (no data)")
        ax.set_xlabel(WORKER_ID_LABEL)
        ax.set_ylabel("Total logical jobs")
        ax.grid(True, axis="y", linestyle="--", alpha=0.3)
        return
    sorted_items = sorted(totals.items(), key=lambda item: item[1], reverse=True)
    workers = [str(worker) for worker, _ in sorted_items]
    values = [item[1] for item in sorted_items]
    bars = ax.bar(workers, values, color="#17becf", edgecolor="black")
    ax.set_title("Worker task distribution")
    ax.set_xlabel(WORKER_ID_LABEL)
    ax.set_ylabel("Total logical jobs")
    ax.grid(True, axis="y", linestyle="--", alpha=0.3)
    if bars:
        ax.bar_label(
            bars, labels=[str(v) for v in values], padding=2, fontsize="x-small"
        )


def load_plot_inputs(args: argparse.Namespace) -> PlotInputs | None:
    csv_path = args.summary_csv
    if not csv_path.exists():
        print(f"error: CSV not found: {csv_path}", file=sys.stderr)
        return None

    try:
        data = load_latencies(csv_path)
    except ValueError as exc:
        print(f"error: {exc}", file=sys.stderr)
        return None

    (
        all_ids,
        all_lat,
        all_workers,
        all_sizes,
        all_jobs,
        all_breakdowns,
        all_arrivals,
    ) = filter_latencies(data)
    (
        cpu_ids,
        cpu_lat,
        cpu_workers,
        cpu_sizes,
        cpu_jobs,
        cpu_breakdowns,
        cpu_arrivals,
    ) = filter_latencies(data, worker_type="cpu")
    (
        gpu_ids,
        gpu_lat,
        gpu_workers,
        gpu_sizes,
        gpu_jobs,
        gpu_breakdowns,
        gpu_arrivals,
    ) = filter_latencies(data, worker_type="cuda")
    worker_type_by_id = {batch_id: worker_type for batch_id, _, worker_type, *_ in data}
    all_worker_types = [
        worker_type_by_id.get(batch_id, "unknown") for batch_id in all_ids
    ]
    batch_times = _compute_batch_times(all_arrivals)

    return PlotInputs(
        all_ids,
        all_lat,
        all_workers,
        all_sizes,
        all_jobs,
        all_breakdowns,
        all_arrivals,
        cpu_ids,
        cpu_lat,
        cpu_workers,
        cpu_sizes,
        cpu_jobs,
        cpu_breakdowns,
        cpu_arrivals,
        gpu_ids,
        gpu_lat,
        gpu_workers,
        gpu_sizes,
        gpu_jobs,
        gpu_breakdowns,
        gpu_arrivals,
        all_worker_types,
        batch_times,
        csv_path,
    )


def _plot_latency_overview(
    axes: list[plt.Axes],
    all_ids: Sequence[int],
    all_lat: Sequence[float],
    all_sizes: Sequence[int],
    cpu_ids: Sequence[int],
    cpu_lat: Sequence[float],
    gpu_ids: Sequence[int],
    gpu_lat: Sequence[float],
    worker_types: Sequence[str],
    batch_times: Sequence[float | None],
    summary_path: Path,
) -> None:
    scatter_with_size(
        axes[0],
        all_ids,
        all_lat,
        all_sizes,
        "Latency vs batch size (multidim)",
        worker_types=worker_types,
    )
    _overlay_congestion_zones(axes[0], all_ids, batch_times, summary_path)

    has_worker_data = False
    if gpu_ids:
        axes[1].scatter(
            gpu_ids,
            gpu_lat,
            s=14,
            alpha=0.7,
            c=GPU_COLOR,
            label="GPU",
        )
        has_worker_data = True
    if cpu_ids:
        axes[1].scatter(
            cpu_ids,
            cpu_lat,
            s=14,
            alpha=0.7,
            c=CPU_COLOR,
            label="CPU",
        )
        has_worker_data = True
    if not has_worker_data and all_ids:
        axes[1].scatter(
            all_ids,
            all_lat,
            s=14,
            alpha=0.7,
            c="#7f7f7f",
            label="All workers",
        )
        has_worker_data = True
    axes[1].set_title(
        "All workers (CPU + GPU)"
        if has_worker_data
        else "All workers (CPU + GPU) (no data)"
    )
    axes[1].set_xlabel(BATCH_ID_LABEL)
    axes[1].set_ylabel(LATENCY_MS_LABEL)
    axes[1].grid(True, linestyle="--", alpha=0.4)
    if has_worker_data:
        axes[1].legend(loc=LEGEND_LOC_UPPER_RIGHT, fontsize="small")

    x_limits = axes[1].get_xlim() if all_ids else None
    y_limits = axes[1].get_ylim() if all_ids else None
    if all_ids and x_limits and y_limits:
        axes[0].set_xlim(x_limits)
        axes[0].set_ylim(y_limits)

    scatter_plot(axes[2], cpu_ids, cpu_lat, "CPU workers only", color=CPU_COLOR)
    if all_ids and x_limits and y_limits:
        axes[2].set_xlim(x_limits)
        axes[2].set_ylim(y_limits)
    scatter_plot(axes[3], gpu_ids, gpu_lat, "GPU workers only")


def _plot_cumulative_and_throughput(
    ax_cumulative: plt.Axes,
    ax_throughput: plt.Axes,
    all_ids: Sequence[int],
    all_lat: Sequence[float],
    all_jobs: Sequence[int],
) -> None:
    cum_ids, cum_vals = compute_cumulative_latency(all_ids, all_lat)
    if cum_ids:
        ax_cumulative.plot(cum_ids, cum_vals, color="teal")
        ax_cumulative.set_title("Cumulative latency vs batch ID")
        ax_cumulative.set_xlabel(BATCH_ID_LABEL)
        ax_cumulative.set_ylabel("Cumulative latency (ms)")
        ax_cumulative.grid(True, linestyle="--", alpha=0.3)
    else:
        ax_cumulative.set_title("Cumulative latency vs batch ID (no data)")
        ax_cumulative.grid(True, linestyle="--", alpha=0.3)

    throughput_ids, throughput_vals = compute_throughput(
        all_ids, all_jobs, window_size=THROUGHPUT_WINDOW
    )
    if throughput_ids:
        ax_throughput.plot(
            throughput_ids,
            throughput_vals,
            color="#9467bd",
            label=f"window={THROUGHPUT_WINDOW} batches",
        )
        ax_throughput.set_title("Throughput vs batch ID (requests per window)")
        ax_throughput.set_xlabel(BATCH_ID_LABEL)
        ax_throughput.set_ylabel("Average logical jobs")
        ax_throughput.grid(True, linestyle="--", alpha=0.3)
        ax_throughput.legend(loc=LEGEND_LOC_LOWER_RIGHT, fontsize="small")
    else:
        ax_throughput.set_title("Throughput vs batch ID (no data)")
        ax_throughput.grid(True, linestyle="--", alpha=0.3)


def _plot_latency_trends(
    ax_sla: plt.Axes,
    ax_avg: plt.Axes,
    ax_cpu_roll: plt.Axes,
    ax_gpu_roll: plt.Axes,
    all_ids: Sequence[int],
    all_lat: Sequence[float],
    cpu_ids: Sequence[int],
    cpu_lat: Sequence[float],
    gpu_ids: Sequence[int],
    gpu_lat: Sequence[float],
) -> None:
    plot_sla_coverage(ax_sla, all_ids, all_lat)

    avg_ids, avg_vals = compute_moving_average(all_ids, all_lat, window=ROLLING_WINDOW)
    if avg_ids:
        ax_avg.plot(avg_ids, avg_vals, color="purple")
        ax_avg.set_title(f"Rolling average latency (window={ROLLING_WINDOW})")
        ax_avg.set_xlabel(BATCH_ID_LABEL)
        ax_avg.set_ylabel(LATENCY_MS_LABEL)
        ax_avg.grid(True, linestyle="--", alpha=0.3)
    else:
        ax_avg.set_title("Rolling average latency (no data)")
        ax_avg.grid(True, linestyle="--", alpha=0.3)

    plot_rolling_percentiles(
        ax_cpu_roll, cpu_ids, cpu_lat, "CPU", window_size=ROLLING_WINDOW
    )
    plot_rolling_percentiles(
        ax_gpu_roll, gpu_ids, gpu_lat, "GPU", window_size=ROLLING_WINDOW
    )


def _plot_phase_views(
    axes: list[plt.Axes],
    all_ids: Sequence[int],
    all_sizes: Sequence[int],
    all_breakdowns: Sequence[Tuple[float, ...]],
    gpu_sizes: Sequence[int],
    gpu_breakdowns: Sequence[Tuple[float, ...]],
    cpu_sizes: Sequence[int],
    cpu_breakdowns: Sequence[Tuple[float, ...]],
    all_workers: Sequence[int],
) -> None:
    plot_latency_stack(axes[0], all_ids, all_breakdowns)

    plot_phase_heatmap(axes[1], all_sizes, all_breakdowns)
    plot_phase_heatmap(axes[2], gpu_sizes, gpu_breakdowns)
    gpu_title = "Phase heatmap (avg ms) - GPU"
    axes[2].set_title(
        gpu_title if gpu_sizes and gpu_breakdowns else f"{gpu_title} (no data)"
    )
    plot_phase_heatmap(axes[3], cpu_sizes, cpu_breakdowns)
    cpu_title = "Phase heatmap (avg ms) - CPU"
    axes[3].set_title(
        cpu_title if cpu_sizes and cpu_breakdowns else f"{cpu_title} (no data)"
    )

    plot_worker_phase_heatmap(axes[4], all_workers, all_breakdowns)

    plot_phase_correlation(axes[5], all_breakdowns)
    plot_phase_correlation(
        axes[6],
        gpu_breakdowns,
        title="Phase correlation heatmap - GPU",
    )
    plot_phase_correlation(
        axes[7],
        cpu_breakdowns,
        title="Phase correlation heatmap - CPU",
    )

    plot_phase_waterfall(axes[8], cpu_breakdowns, gpu_breakdowns)
    plot_phase_pareto(axes[9], all_breakdowns)


def _fit_and_plot_correlation(
    ax: plt.Axes, sizes: Sequence[int], lat: Sequence[float], *, color: str
) -> bool:
    if sizes and lat:
        ax.scatter(sizes, lat, alpha=0.6, color=color)
        if len(sizes) >= 2:
            sorted_pairs = sorted(zip(sizes, lat))
            xs = np.array([p[0] for p in sorted_pairs])
            ys = np.array([p[1] for p in sorted_pairs])
            coeffs = np.polyfit(xs, ys, deg=1)
            fit_x = np.linspace(xs.min(), xs.max(), num=200)
            fit_y = np.polyval(coeffs, fit_x)
            ax.plot(fit_x, fit_y, color="black", linestyle="--")
        return True
    return False


def _plot_size_correlations(
    ax_all: plt.Axes,
    ax_gpu: plt.Axes,
    ax_cpu: plt.Axes,
    all_sizes: Sequence[int],
    all_lat: Sequence[float],
    gpu_sizes: Sequence[int],
    gpu_lat: Sequence[float],
    cpu_sizes: Sequence[int],
    cpu_lat: Sequence[float],
) -> None:
    corr_xlim = corr_ylim = None
    if _fit_and_plot_correlation(ax_all, all_sizes, all_lat, color="#17becf"):
        ax_all.set_title("Latency vs batch size (correlation)")
        corr_xlim = ax_all.get_xlim()
        corr_ylim = ax_all.get_ylim()
    else:
        ax_all.set_title("Latency vs batch size (correlation) (no data)")
    ax_all.set_xlabel(BATCH_SIZE_LABEL)
    ax_all.set_ylabel(LATENCY_MS_LABEL)
    ax_all.grid(True, linestyle="--", alpha=0.3)

    if _fit_and_plot_correlation(ax_gpu, gpu_sizes, gpu_lat, color=GPU_COLOR):
        ax_gpu.set_title("Latency vs batch size (correlation) - GPU")
    else:
        ax_gpu.set_title("Latency vs batch size (correlation) - GPU (no data)")
    ax_gpu.set_xlabel(BATCH_SIZE_LABEL)
    ax_gpu.set_ylabel(LATENCY_MS_LABEL)
    ax_gpu.grid(True, linestyle="--", alpha=0.3)
    if corr_xlim and corr_ylim:
        ax_gpu.set_xlim(corr_xlim)
        ax_gpu.set_ylim(corr_ylim)

    if _fit_and_plot_correlation(ax_cpu, cpu_sizes, cpu_lat, color=CPU_COLOR):
        ax_cpu.set_title("Latency vs batch size (correlation) - CPU")
    else:
        ax_cpu.set_title("Latency vs batch size (correlation) - CPU (no data)")
    ax_cpu.set_xlabel(BATCH_SIZE_LABEL)
    ax_cpu.set_ylabel(LATENCY_MS_LABEL)
    ax_cpu.grid(True, linestyle="--", alpha=0.3)
    if corr_xlim and corr_ylim:
        ax_cpu.set_xlim(corr_xlim)
        ax_cpu.set_ylim(corr_ylim)


def _plot_size_distribution(
    ax: plt.Axes,
    all_sizes: Sequence[int],
    cpu_sizes: Sequence[int],
    gpu_sizes: Sequence[int],
) -> None:
    size_counts_all = Counter(all_sizes)
    size_counts_cpu = Counter(cpu_sizes)
    size_counts_gpu = Counter(gpu_sizes)
    unique_sizes = sorted(
        set(size_counts_all.keys())
        | set(size_counts_cpu.keys())
        | set(size_counts_gpu.keys())
    )
    if not unique_sizes:
        ax.set_title("Batch size distribution (no data)")
        ax.set_xlabel(BATCH_SIZE_LABEL)
        ax.set_ylabel("Count")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    series = []
    if size_counts_all:
        series.append(("All", size_counts_all, "gray", 0.7))
    if size_counts_cpu:
        series.append(("CPU", size_counts_cpu, CPU_COLOR, 0.6))
    if size_counts_gpu:
        series.append(("GPU", size_counts_gpu, "blue", 0.6))
    bar_groups = max(1, len(series))
    width = 0.8 / bar_groups
    positions = np.arange(len(unique_sizes))
    handles = []
    for idx, (label, counts, color, alpha) in enumerate(series):
        offset = (idx - (bar_groups - 1) / 2) * width
        heights = [counts.get(size, 0) for size in unique_sizes]
        bars = ax.bar(
            positions + offset,
            heights,
            width=width * 0.9,
            label=label,
            color=color,
            alpha=alpha,
            edgecolor="black",
        )
        handles.append(bars)
    ax.set_xticks(positions, [str(size) for size in unique_sizes])
    ax.set_title("Batch size distribution")
    ax.set_xlabel(BATCH_SIZE_LABEL)
    ax.set_ylabel("Count")
    if handles:
        ax.legend()
    ax.grid(True, linestyle="--", alpha=0.3)


def _plot_latency_distributions(
    ax_violin: plt.Axes,
    ax_worker_cdf: plt.Axes,
    ax_worker_grid: plt.Axes,
    ax_batch_cdf: plt.Axes,
    cpu_lat: Sequence[float],
    gpu_lat: Sequence[float],
    all_workers: Sequence[int],
    all_lat: Sequence[float],
    all_sizes: Sequence[int],
) -> None:
    violin_data = []
    labels = []
    if cpu_lat:
        violin_data.append(cpu_lat)
        labels.append("CPU")
    if gpu_lat:
        violin_data.append(gpu_lat)
        labels.append("GPU")
    if violin_data:
        ax_violin.violinplot(violin_data, showmeans=True, showmedians=False)
        ax_violin.set_xticks(range(1, len(labels) + 1), labels)
        ax_violin.set_title("Latency distribution (violin plot)")
        ax_violin.set_ylabel(LATENCY_MS_LABEL)
        ax_violin.grid(True, linestyle="--", alpha=0.3)
    else:
        ax_violin.set_title("Latency distribution (no data)")
        ax_violin.grid(True, linestyle="--", alpha=0.3)

    worker_cdf_data: List[Sequence[float]] = []
    worker_cdf_labels: List[str] = []
    if cpu_lat:
        worker_cdf_data.append(cpu_lat)
        worker_cdf_labels.append("CPU")
    if gpu_lat:
        worker_cdf_data.append(gpu_lat)
        worker_cdf_labels.append("GPU")
    plot_latency_cdf(
        ax_worker_cdf,
        worker_cdf_data,
        worker_cdf_labels,
        "Latency CDF by worker type",
    )

    plot_worker_cdf_grid(ax_worker_grid, all_workers, all_lat)

    size_latencies: dict[int, List[float]] = {}
    for size, latency in zip(all_sizes, all_lat):
        size_latencies.setdefault(size, []).append(latency)
    top_sizes = sorted(
        size_latencies.items(), key=lambda item: len(item[1]), reverse=True
    )[:MAX_BATCH_CDF_SERIES]
    batch_cdf_data = [item[1] for item in top_sizes]
    batch_cdf_labels = [f"size {item[0]}" for item in top_sizes]
    plot_latency_cdf(
        ax_batch_cdf,
        batch_cdf_data,
        batch_cdf_labels,
        "Latency CDF by batch size",
    )


def _plot_worker_views(
    ax_util: plt.Axes,
    ax_box: plt.Axes,
    ax_tasks: plt.Axes,
    ax_heatmap: plt.Axes,
    all_workers: Sequence[int],
    all_breakdowns: Sequence[Tuple[float, ...]],
    all_ids: Sequence[int],
    all_lat: Sequence[float],
    all_jobs: Sequence[int],
) -> None:
    plot_worker_phase_utilization(ax_util, all_workers, all_breakdowns)
    plot_worker_boxplots(ax_box, all_workers, all_lat)
    plot_worker_task_distribution(ax_tasks, all_workers, all_jobs)
    plot_worker_time_heatmap(ax_heatmap, all_workers, all_ids, all_lat)


def _plot_arrival_views(
    ax_timeline: plt.Axes,
    ax_rate: plt.Axes,
    all_arrivals: Sequence[Sequence[int]],
) -> None:
    plot_request_arrival_timeline(ax_timeline, all_arrivals)
    plot_request_arrival_rate(ax_rate, all_arrivals)


def _plot_radar(
    fig: plt.Figure,
    axes: list[plt.Axes],
    all_workers: Sequence[int],
    all_breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    axes[34].remove()
    axes[34] = fig.add_subplot(35, 1, 35, projection="polar")
    plot_worker_radar(axes[34], all_workers, all_breakdowns)


def render_plots(
    fig: plt.Figure,
    axes: list[plt.Axes],
    inputs: PlotInputs,
    output_path: Path | None,
) -> None:
    (
        all_ids,
        all_lat,
        all_workers,
        all_sizes,
        all_jobs,
        all_breakdowns,
        all_arrivals,
        cpu_ids,
        cpu_lat,
        cpu_workers,
        cpu_sizes,
        cpu_jobs,
        cpu_breakdowns,
        cpu_arrivals,
        gpu_ids,
        gpu_lat,
        gpu_workers,
        gpu_sizes,
        gpu_jobs,
        gpu_breakdowns,
        gpu_arrivals,
        all_worker_types,
        batch_times,
        summary_path,
    ) = inputs

    _plot_latency_overview(
        axes,
        all_ids,
        all_lat,
        all_sizes,
        cpu_ids,
        cpu_lat,
        gpu_ids,
        gpu_lat,
        all_worker_types,
        batch_times,
        summary_path,
    )

    _plot_cumulative_and_throughput(axes[4], axes[5], all_ids, all_lat, all_jobs)

    _plot_latency_trends(
        axes[6],
        axes[7],
        axes[8],
        axes[9],
        all_ids,
        all_lat,
        cpu_ids,
        cpu_lat,
        gpu_ids,
        gpu_lat,
    )

    _plot_phase_views(
        axes[10:20],
        all_ids,
        all_sizes,
        all_breakdowns,
        gpu_sizes,
        gpu_breakdowns,
        cpu_sizes,
        cpu_breakdowns,
        all_workers,
    )

    _plot_size_correlations(
        axes[20],
        axes[21],
        axes[22],
        all_sizes,
        all_lat,
        gpu_sizes,
        gpu_lat,
        cpu_sizes,
        cpu_lat,
    )

    _plot_size_distribution(axes[23], all_sizes, cpu_sizes, gpu_sizes)

    _plot_latency_distributions(
        axes[24],
        axes[25],
        axes[26],
        axes[27],
        cpu_lat,
        gpu_lat,
        all_workers,
        all_lat,
        all_sizes,
    )

    _plot_worker_views(
        axes[28],
        axes[29],
        axes[30],
        axes[31],
        all_workers,
        all_breakdowns,
        all_ids,
        all_lat,
        all_jobs,
    )

    _plot_arrival_views(axes[32], axes[33], all_arrivals)
    _plot_radar(fig, axes, all_workers, all_breakdowns)

    fig.subplots_adjust(hspace=0.6, top=0.98, bottom=0.02)

    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(output_path, dpi=150)
        print(f"Saved plots to {output_path}")
    else:
        plt.show()


def create_figure() -> tuple[plt.Figure, list[plt.Axes]]:
    fig, axes_array = plt.subplots(35, 1, figsize=(12, 136), sharex=False)
    return fig, list(axes_array)


def main() -> int:
    args = parse_args()
    inputs = load_plot_inputs(args)
    if inputs is None:
        return 1

    fig, axes = create_figure()
    render_plots(fig, axes, inputs, args.output)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
