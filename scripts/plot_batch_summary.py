#!/usr/bin/env python3
"""
Plot batch latencies from batching_trace_summary.csv.

This script expects the CSV generated by batching tracing (when trace_enabled is
true). It produces three scatter plots:

1. Combined batches (CPU + GPU)
2. CPU-only batches
3. GPU-only batches

Usage:
  ./scripts/plot_batch_summary.py PATH_TO/batching_trace_summary.csv
  ./scripts/plot_batch_summary.py PATH_TO/file.csv --output plots.png
"""

from __future__ import annotations

import argparse
import csv
import sys
from collections import Counter, deque
from itertools import cycle
from pathlib import Path
from typing import Iterable, List, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.gridspec import GridSpecFromSubplotSpec
from mpl_toolkits.axes_grid1.inset_locator import inset_axes

PHASE_LABELS = [
    "queue",
    "batch",
    "submit",
    "scheduling",
    "codelet",
    "inference",
    "callback",
]

PHASE_COLORS = {
    "queue": "#1f77b4",
    "batch": "#ff7f0e",
    "submit": "#2ca02c",
    "scheduling": "#d62728",
    "codelet": "#9467bd",
    "inference": "#8c564b",
    "callback": "#e377c2",
}

ROLLING_WINDOW = 50
THROUGHPUT_WINDOW = 100
MAX_BATCH_CDF_SERIES = 4
PHASE_INDEX = {label: idx for idx, label in enumerate(PHASE_LABELS)}
SLA_THRESHOLDS_MS = (50.0, 100.0, 200.0)
MAX_WORKER_CDFS = 6

BatchRow = Tuple[
    int,  # batch_id
    float,  # total latency (ms)
    str,  # worker type
    int,  # worker_id
    int,  # batch size
    int,  # logical job count
    Tuple[float, ...],  # phase breakdown
    Tuple[int, ...],  # request arrival timestamps (us)
]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Plot batch latencies from batching_trace_summary.csv."
    )
    parser.add_argument(
        "summary_csv",
        type=Path,
        help="Path to batching_trace_summary.csv",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        help=(
            "Optional output image path (PNG/JPG/etc.). "
            "If omitted the plots are shown interactively."
        ),
    )
    return parser.parse_args()


def parse_request_arrival_field(field: str | None) -> Tuple[int, ...]:
    if not field:
        return ()
    values: List[int] = []
    for raw in field.split(";"):
        token = raw.strip()
        if not token:
            continue
        try:
            values.append(int(token))
        except ValueError as exc:
            raise ValueError(
                f"Invalid request_arrival_us entry '{raw}' in field '{field}'"
            ) from exc
    return tuple(values)


def load_latencies(
    csv_path: Path,
) -> List[BatchRow]:
    batches: List[BatchRow] = []
    with csv_path.open(newline="") as handle:
        reader = csv.DictReader(handle)
        required = {
            "batch_id",
            "total_ms",
            "worker_type",
            "worker_id",
            "batch_size",
        }
        required.update(f"{label}_ms" for label in PHASE_LABELS)
        missing = sorted(required - set(reader.fieldnames or []))
        if missing:
            raise ValueError(
                f"{csv_path} is missing required columns: {', '.join(missing)}"
            )
        for row in reader:
            try:
                batch_id = int(row["batch_id"])
                latency = float(row["total_ms"])
            except ValueError as exc:
                raise ValueError(f"Invalid numeric values in row: {row}") from exc
            worker_type = (row.get("worker_type") or "unknown").strip().lower()
            worker_id = int(row["worker_id"])
            batch_size = int(row["batch_size"])
            logical_jobs_field = row.get("logical_jobs")
            logical_jobs = (
                int(logical_jobs_field)
                if logical_jobs_field not in (None, "")
                else batch_size
            )
            breakdown = tuple(float(row[f"{label}_ms"]) for label in PHASE_LABELS)
            arrivals_field = row.get("request_arrival_us")
            arrivals = parse_request_arrival_field(arrivals_field)
            batches.append(
                (
                    batch_id,
                    latency,
                    worker_type,
                    worker_id,
                    batch_size,
                    logical_jobs,
                    breakdown,
                    arrivals,
                )
            )
    return batches


def filter_latencies(
    data: Iterable[BatchRow],
    *,
    worker_type: str | None = None,
) -> Tuple[
    List[int],
    List[float],
    List[int],
    List[int],
    List[int],
    List[Tuple[float, ...]],
    List[Tuple[int, ...]],
]:
    ids: List[int] = []
    latencies: List[float] = []
    worker_ids: List[int] = []
    batch_sizes: List[int] = []
    logical_jobs: List[int] = []
    breakdowns: List[Tuple[float, ...]] = []
    arrivals: List[Tuple[int, ...]] = []
    for (
        batch_id,
        latency,
        device,
        worker_id,
        batch_size,
        logical_job_count,
        breakdown,
        request_arrivals,
    ) in data:
        if worker_type is None or device == worker_type:
            ids.append(batch_id)
            latencies.append(latency)
            worker_ids.append(worker_id)
            batch_sizes.append(batch_size)
            logical_jobs.append(logical_job_count)
            breakdowns.append(breakdown)
            arrivals.append(request_arrivals)
    return ids, latencies, worker_ids, batch_sizes, logical_jobs, breakdowns, arrivals


def flatten_request_arrival_seconds(
    arrival_sequences: Sequence[Sequence[int]],
) -> List[float]:
    events: List[int] = []
    for sequence in arrival_sequences:
        for timestamp in sequence:
            if timestamp > 0:
                events.append(timestamp)
    if not events:
        return []
    events.sort()
    start = events[0]
    relative_seconds = [
        (timestamp - start) / 1_000_000.0 for timestamp in events if timestamp >= start
    ]
    return relative_seconds


def plot_latency_stack(
    ax,
    batch_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    if not batch_ids or not breakdowns:
        ax.set_title("Latency composition per batch (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Latency contribution (ms)")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    labels = PHASE_LABELS
    components = list(map(list, zip(*breakdowns)))
    bottom = [0.0] * len(batch_ids)
    ax.set_title("Latency composition per batch")
    for values, label in zip(components, labels):
        ax.bar(batch_ids, values, bottom=bottom, label=label, width=0.6)
        bottom = [b + v for b, v in zip(bottom, values)]
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Latency contribution (ms)")
    ax.legend(loc="upper right", fontsize="small")
    ax.grid(True, linestyle="--", alpha=0.3)


def scatter_plot(
    ax, x: List[int], y: List[float], title: str, *, color: str | None = None
) -> None:
    if not x:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Latency (ms)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    ax.scatter(x, y, s=14, alpha=0.7, c=color)
    ax.set_title(title)
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Latency (ms)")
    ax.grid(True, linestyle="--", alpha=0.4)


def plot_violin(
    ax,
    data: Sequence[float],
    label: str,
    color: str | None = None,
) -> None:
    if not data:
        ax.set_title(f"{label} (no data)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    parts = ax.violinplot(data, showmeans=True, showmedians=False)
    if color:
        for pc in parts["bodies"]:
            pc.set_facecolor(color)
            pc.set_edgecolor("black")
        parts["cbars"].set_edgecolor(color)
        parts["cmins"].set_edgecolor(color)
        parts["cmaxes"].set_edgecolor(color)
    ax.set_title(label)
    ax.grid(True, linestyle="--", alpha=0.4)


def scatter_with_size(
    ax,
    x: Sequence[int],
    y: Sequence[float],
    sizes: Sequence[int],
    title: str,
    worker_types: Sequence[str] | None = None,
) -> None:
    if not x or not y or not sizes:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Latency (ms)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    max_size = max(sizes)
    min_size = max(1, min(sizes))
    scale = []
    for value in sizes:
        norm = value / max_size if max_size > 0 else 0.0
        scale.append(50 + norm * 200)
    scatter = ax.scatter(
        x,
        y,
        s=scale,
        c=sizes,
        cmap="viridis",
        alpha=0.7,
    )
    if worker_types and len(worker_types) == len(x):
        cpu_x: List[int] = []
        cpu_y: List[float] = []
        gpu_x: List[int] = []
        gpu_y: List[float] = []
        other_x: List[int] = []
        other_y: List[float] = []
        for px, py, worker in zip(x, y, worker_types):
            kind = (worker or "").lower()
            if kind == "cpu":
                cpu_x.append(px)
                cpu_y.append(py)
            elif kind in ("cuda", "gpu"):
                gpu_x.append(px)
                gpu_y.append(py)
            else:
                other_x.append(px)
                other_y.append(py)
        overlay_kwargs = {"s": 5, "alpha": 0.95, "zorder": 3, "marker": "o"}
        overlay_handles = []
        if gpu_x:
            handle = ax.scatter(
                gpu_x, gpu_y, color="#1f77b4", label="GPU", **overlay_kwargs
            )
            overlay_handles.append(handle)
        if cpu_x:
            handle = ax.scatter(
                cpu_x, cpu_y, color="#d62728", label="CPU", **overlay_kwargs
            )
            overlay_handles.append(handle)
        if other_x:
            handle = ax.scatter(
                other_x, other_y, color="#7f7f7f", label="Other", **overlay_kwargs
            )
            overlay_handles.append(handle)
        if overlay_handles:
            ax.legend(handles=overlay_handles, loc="upper left", fontsize="small")
    ax.set_title(title)
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Latency (ms)")
    ax.grid(True, linestyle="--", alpha=0.4)
    # Place colorbar outside the axes to avoid shrinking or covering data.
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc="lower left",
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(scatter, cax=cax)
    cbar.set_label("Batch size")


def plot_worker_boxplots(
    ax,
    worker_ids: Sequence[int],
    latencies: Sequence[float],
) -> None:
    data = {}
    for worker, latency in zip(worker_ids, latencies):
        if worker < 0:
            continue
        data.setdefault(worker, []).append(latency)
    if not data:
        ax.set_title("Worker latency boxplots (no data)")
        ax.set_xlabel("Worker ID")
        ax.set_ylabel("Latency (ms)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    sorted_items = sorted(data.items(), key=lambda item: item[0])
    ax.boxplot(
        [item[1] for item in sorted_items],
        labels=[item[0] for item in sorted_items],
        showmeans=True,
    )
    ax.set_title("Worker latency boxplots")
    ax.set_xlabel("Worker ID")
    ax.set_ylabel("Latency (ms)")
    ax.grid(True, linestyle="--", alpha=0.4)


def plot_worker_radar(
    ax,
    worker_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_workers: int = 4,
) -> None:
    contributions: dict[int, np.ndarray] = {}
    counts: dict[int, int] = {}
    for worker, phases in zip(worker_ids, breakdowns):
        if worker < 0:
            continue
        contributions.setdefault(worker, np.zeros(len(PHASE_LABELS)))
        contributions[worker] += np.array(phases)
        counts[worker] = counts.get(worker, 0) + 1

    if not contributions:
        ax.set_title("Worker radar (no data)")
        ax.set_axis_off()
        return

    averages = []
    for worker, total in contributions.items():
        average = total / counts[worker]
        averages.append((worker, average))
    averages.sort(key=lambda item: counts[item[0]], reverse=True)
    averages = averages[:max_workers]

    angles = np.linspace(0, 2 * np.pi, len(PHASE_LABELS), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))
    color_cycle = cycle(["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728"])
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_xticks(angles[:-1], PHASE_LABELS)
    ax.set_title("Worker radar (avg phase ms)")

    for worker, avg in averages:
        values = np.concatenate((avg, [avg[0]]))
        color = next(color_cycle)
        ax.plot(angles, values, label=f"worker {worker}", color=color)
        ax.fill(angles, values, alpha=0.1, color=color)

    ax.legend(loc="upper right", bbox_to_anchor=(1.2, 1.1), fontsize="small")


def plot_worker_phase_utilization(
    ax,
    worker_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    contributions: dict[int, np.ndarray] = {}
    for worker, phases in zip(worker_ids, breakdowns):
        if worker < 0:
            continue
        contributions.setdefault(worker, np.zeros(len(PHASE_LABELS)))
        contributions[worker] += np.array(phases)
    if not contributions:
        ax.set_title("Worker phase utilization (no data)")
        ax.set_xlabel("Total time (ms)")
        ax.set_ylabel("Worker ID")
        ax.grid(True, axis="x", linestyle="--", alpha=0.3)
        return
    sorted_items = sorted(
        contributions.items(), key=lambda item: item[1].sum(), reverse=True
    )
    worker_labels = [str(worker) for worker, _ in sorted_items]
    y_pos = np.arange(len(sorted_items))
    left = np.zeros(len(sorted_items), dtype=float)
    legend_added: set[str] = set()
    for idx, phase in enumerate(PHASE_LABELS):
        values = [item[1][idx] for item in sorted_items]
        color = PHASE_COLORS.get(phase, None)
        ax.barh(
            y_pos,
            values,
            left=left,
            color=color,
            label=phase if phase not in legend_added else None,
        )
        left += np.array(values)
        legend_added.add(phase)
    ax.set_yticks(y_pos, worker_labels)
    ax.set_xlabel("Total time (ms)")
    ax.set_ylabel("Worker ID")
    ax.set_title("Worker phase utilization")
    ax.grid(True, axis="x", linestyle="--", alpha=0.3)
    ax.legend(loc="upper right", fontsize="small")


def plot_worker_time_heatmap(
    ax,
    worker_ids: Sequence[int],
    batch_ids: Sequence[int],
    latencies: Sequence[float],
    max_workers: int = 12,
    bucket_size: int = 100,
) -> None:
    if not worker_ids or not batch_ids or not latencies:
        ax.set_title("Worker-time heatmap (no data)")
        ax.set_xlabel("Batch bucket")
        ax.set_ylabel("Worker ID")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    sorted_workers = sorted(set(worker_ids), key=lambda wid: (wid < 0, wid))[
        :max_workers
    ]
    worker_index = {wid: idx for idx, wid in enumerate(sorted_workers)}
    bucket_map: dict[tuple[int, int], list[float]] = {}
    for wid, bid, latency in zip(worker_ids, batch_ids, latencies):
        if wid not in worker_index or wid < 0:
            continue
        bucket = bid // bucket_size
        bucket_map.setdefault((wid, bucket), []).append(latency)

    if not bucket_map:
        ax.set_title("Worker-time heatmap (no data)")
        ax.set_xlabel("Batch bucket")
        ax.set_ylabel("Worker ID")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    buckets = sorted({bucket for _, bucket in bucket_map.keys()})
    heatmap = np.full((len(sorted_workers), len(buckets)), np.nan)
    bucket_index = {bucket: idx for idx, bucket in enumerate(buckets)}
    for (wid, bucket), samples in bucket_map.items():
        row = worker_index[wid]
        col = bucket_index[bucket]
        heatmap[row, col] = np.mean(samples)

    im = ax.imshow(heatmap, aspect="auto", cmap="plasma", origin="lower")
    ax.set_xticks(
        range(len(buckets)), [str(bucket * bucket_size) for bucket in buckets]
    )
    ax.set_yticks(range(len(sorted_workers)), [str(w) for w in sorted_workers])
    ax.set_xlabel("Batch ID bucket")
    ax.set_ylabel("Worker ID")
    ax.set_title("Worker-time heatmap (avg latency)")
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc="lower left",
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label("Latency (ms)")


def plot_phase_heatmap(
    ax,
    batch_sizes: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_columns: int = 12,
) -> None:
    if not batch_sizes or not breakdowns:
        ax.set_title("Phase heatmap (no data)")
        ax.set_xlabel("Batch size")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return

    counter = Counter(batch_sizes)
    top_sizes = [size for size, _ in counter.most_common(max_columns)]
    top_sizes.sort()
    if not top_sizes:
        ax.set_title("Phase heatmap (no data)")
        ax.set_xlabel("Batch size")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return

    index = {size: idx for idx, size in enumerate(top_sizes)}
    totals = np.zeros((len(PHASE_LABELS), len(top_sizes)), dtype=float)
    counts = np.zeros_like(totals)

    for size, phases in zip(batch_sizes, breakdowns):
        col = index.get(size)
        if col is None:
            continue
        for row, value in enumerate(phases):
            totals[row, col] += value
            counts[row, col] += 1

    with np.errstate(invalid="ignore"):
        averages = np.divide(totals, counts, where=counts > 0)

    masked = np.ma.masked_invalid(averages)
    im = ax.imshow(masked, aspect="auto", cmap="magma", origin="lower")
    ax.set_xticks(range(len(top_sizes)), [str(size) for size in top_sizes])
    ax.set_yticks(range(len(PHASE_LABELS)), PHASE_LABELS)
    ax.set_xlabel("Batch size")
    ax.set_ylabel("Phase")
    ax.set_title("Phase heatmap (avg ms) - All workers")
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc="lower left",
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label("Average duration (ms)")


def plot_worker_phase_heatmap(
    ax,
    worker_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_workers: int = 12,
) -> None:
    totals: dict[int, np.ndarray] = {}
    counts: dict[int, int] = {}
    for worker, phases in zip(worker_ids, breakdowns):
        if worker < 0:
            continue
        totals.setdefault(worker, np.zeros(len(PHASE_LABELS), dtype=float))
        totals[worker] += np.array(phases)
        counts[worker] = counts.get(worker, 0) + 1
    if not totals:
        ax.set_title("Phase heatmap by worker (no data)")
        ax.set_xlabel("Worker ID")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    sorted_workers = sorted(
        totals.keys(), key=lambda worker: (counts.get(worker, 0), worker), reverse=True
    )[:max_workers]
    matrix = np.zeros((len(PHASE_LABELS), len(sorted_workers)), dtype=float)
    for col, worker in enumerate(sorted_workers):
        average = totals[worker] / max(1, counts.get(worker, 1))
        matrix[:, col] = average
    im = ax.imshow(matrix, aspect="auto", cmap="magma", origin="lower")
    ax.set_xticks(
        range(len(sorted_workers)), [str(worker) for worker in sorted_workers]
    )
    ax.set_yticks(range(len(PHASE_LABELS)), PHASE_LABELS)
    ax.set_xlabel("Worker ID")
    ax.set_ylabel("Phase")
    ax.set_title("Phase heatmap by worker (avg ms)")
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc="lower left",
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label("Average duration (ms)")


def plot_phase_correlation(
    ax,
    breakdowns: Sequence[Tuple[float, ...]],
    *,
    title: str | None = None,
) -> None:
    base_title = title or "Phase correlation heatmap"
    if not breakdowns:
        ax.set_title(f"{base_title} (no data)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    matrix = np.array(breakdowns, dtype=float)
    if matrix.ndim != 2 or matrix.shape[0] < 2:
        ax.set_title(f"{base_title} (insufficient data)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    with np.errstate(invalid="ignore"):
        corr = np.corrcoef(matrix, rowvar=False)
    if np.all(np.isnan(corr)):
        ax.set_title(f"{base_title} (no variation)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    masked = np.ma.masked_invalid(corr)
    im = ax.imshow(
        masked,
        cmap="coolwarm",
        origin="lower",
        vmin=-1,
        vmax=1,
        aspect="auto",
    )
    ax.set_xticks(range(len(PHASE_LABELS)), PHASE_LABELS, rotation=45, ha="right")
    ax.set_yticks(range(len(PHASE_LABELS)), PHASE_LABELS)
    ax.set_xlabel("Phase")
    ax.set_ylabel("Phase")
    ax.set_title(base_title)
    cax = inset_axes(
        ax,
        width="2%",
        height="70%",
        loc="lower left",
        bbox_to_anchor=(1.02, 0.15, 1, 1),
        bbox_transform=ax.transAxes,
        borderpad=0.0,
    )
    cbar = plt.colorbar(im, cax=cax)
    cbar.set_label("Pearson r")


def plot_phase_pareto(
    ax,
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    if not breakdowns:
        ax.set_title("Phase Pareto (no data)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Average duration (ms)")
        ax.grid(True, axis="y", linestyle="--", alpha=0.3)
        return
    totals = np.zeros(len(PHASE_LABELS), dtype=float)
    for phases in breakdowns:
        totals += np.array(phases)
    averages = totals / len(breakdowns)
    sorted_indices = np.argsort(-averages)
    sorted_labels = [PHASE_LABELS[idx] for idx in sorted_indices]
    sorted_values = averages[sorted_indices]
    ax.bar(sorted_labels, sorted_values, color="#6a4c93")
    ax.set_title("Phase Pareto (avg ms)")
    ax.set_xlabel("Phase")
    ax.set_ylabel("Average duration (ms)")
    ax.grid(True, axis="y", linestyle="--", alpha=0.3)


def plot_phase_waterfall(
    ax,
    cpu_breakdowns: Sequence[Tuple[float, ...]],
    gpu_breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    categories = []
    data = []
    if cpu_breakdowns:
        categories.append("CPU")
        data.append(np.array(cpu_breakdowns))
    if gpu_breakdowns:
        categories.append("GPU")
        data.append(np.array(gpu_breakdowns))
    if not categories:
        ax.set_title("Phase waterfall (no data)")
        ax.set_xlabel("Total time (ms)")
        ax.set_ylabel("Worker type")
        ax.grid(True, axis="x", linestyle="--", alpha=0.3)
        return

    y_positions = np.arange(len(categories))
    legend_added = set()
    for idx, (label, breakdown_array) in enumerate(zip(categories, data)):
        totals = breakdown_array.sum(axis=0)
        left = 0.0
        for phase, value in zip(PHASE_LABELS, totals):
            color = PHASE_COLORS.get(phase, None)
            bar = ax.barh(
                y_positions[idx],
                value,
                left=left,
                color=color,
                label=phase if phase not in legend_added else None,
            )
            left += value
            legend_added.add(phase)
    ax.set_yticks(y_positions, categories)
    ax.set_xlabel("Total time (ms)")
    ax.set_title("Phase waterfall (CPU vs GPU)")
    ax.grid(True, axis="x", linestyle="--", alpha=0.3)
    ax.legend(loc="upper right", fontsize="small")


def compute_moving_average(
    ids: Sequence[int], values: Sequence[float], window: int = 50
) -> Tuple[List[int], List[float]]:
    if not ids or not values:
        return [], []
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    sorted_ids = [pair[0] for pair in sorted_pairs]
    sorted_vals = [pair[1] for pair in sorted_pairs]
    window = max(1, min(window, len(sorted_vals)))
    rolling_ids: List[int] = []
    rolling_vals: List[float] = []
    window_sum = 0.0
    current = deque()
    for batch_id, value in zip(sorted_ids, sorted_vals):
        current.append(value)
        window_sum += value
        if len(current) > window:
            window_sum -= current.popleft()
        rolling_ids.append(batch_id)
        rolling_vals.append(window_sum / len(current))
    return rolling_ids, rolling_vals


def compute_rolling_percentiles(
    ids: Sequence[int],
    values: Sequence[float],
    *,
    window_size: int = 50,
    percentiles: Sequence[int] = (50, 95, 99),
) -> dict[int, Tuple[List[int], List[float]]]:
    percentiles = tuple(percentiles)
    result = {perc: ([], []) for perc in percentiles}
    if not ids or not values:
        return result
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    window = deque()
    window_size = max(1, window_size)
    for batch_id, value in sorted_pairs:
        window.append(float(value))
        if len(window) > window_size:
            window.popleft()
        window_array = np.fromiter(window, dtype=float)
        percentile_values = np.percentile(window_array, percentiles)
        for perc, perc_value in zip(percentiles, percentile_values):
            result[perc][0].append(batch_id)
            result[perc][1].append(float(perc_value))
    return result


def compute_cumulative_latency(
    ids: Sequence[int], values: Sequence[float]
) -> Tuple[List[int], List[float]]:
    if not ids or not values:
        return [], []
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    cum_sum = 0.0
    cum_ids: List[int] = []
    cum_vals: List[float] = []
    for batch_id, latency in sorted_pairs:
        cum_sum += latency
        cum_ids.append(batch_id)
        cum_vals.append(cum_sum)
    return cum_ids, cum_vals


def compute_throughput(
    ids: Sequence[int],
    jobs: Sequence[int],
    window_size: int = THROUGHPUT_WINDOW,
) -> Tuple[List[int], List[float]]:
    if not ids or not jobs:
        return [], []
    sorted_pairs = sorted(zip(ids, jobs), key=lambda pair: pair[0])
    throughput_ids: List[int] = []
    throughput_vals: List[float] = []
    window = deque()
    jobs_sum = 0
    batch_sum = 0
    for batch_id, job_count in sorted_pairs:
        window.append((batch_id, job_count))
        jobs_sum += job_count
        batch_sum += 1
        while window and batch_sum > window_size:
            _, oldest_jobs = window.popleft()
            jobs_sum -= oldest_jobs
            batch_sum -= 1
        throughput_ids.append(batch_id)
        throughput_vals.append(jobs_sum / max(1, batch_sum))
    return throughput_ids, throughput_vals


def plot_request_arrival_timeline(
    ax,
    arrival_sequences: Sequence[Sequence[int]],
) -> None:
    relative_seconds = flatten_request_arrival_seconds(arrival_sequences)
    if not relative_seconds:
        ax.set_title("Request arrival timeline (no data)")
        ax.set_xlabel("Time since first arrival (s)")
        ax.set_ylabel("Cumulative requests")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    counts = np.arange(1, len(relative_seconds) + 1)
    ax.step(relative_seconds, counts, where="post", color="#ff9896")
    ax.set_title("Request arrival timeline")
    ax.set_xlabel("Time since first arrival (s)")
    ax.set_ylabel("Cumulative requests")
    ax.grid(True, linestyle="--", alpha=0.3)
    if relative_seconds:
        duration = relative_seconds[-1]
        if duration > 0:
            avg_rate = counts[-1] / duration
            ax.text(
                0.98,
                0.05,
                f"avg {avg_rate:.1f} req/s",
                transform=ax.transAxes,
                ha="right",
                va="bottom",
                fontsize="small",
                color="#444444",
            )


def plot_request_arrival_rate(
    ax,
    arrival_sequences: Sequence[Sequence[int]],
    *,
    bin_width: float = 1.0,
    max_bins: int = 200,
) -> None:
    relative_seconds = flatten_request_arrival_seconds(arrival_sequences)
    if not relative_seconds:
        ax.set_title("Request arrival rate (no data)")
        ax.set_xlabel("Time since first arrival (s)")
        ax.set_ylabel("Requests/s")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    duration = max(relative_seconds[-1], bin_width)
    bin_width = max(bin_width, 0.1)
    bin_count = min(max_bins, max(1, int(np.ceil(duration / bin_width))))
    bins = np.linspace(0.0, bin_count * bin_width, num=bin_count + 1)
    counts, edges = np.histogram(relative_seconds, bins=bins)
    rates = counts / bin_width
    ax.bar(
        edges[:-1],
        rates,
        width=bin_width * 0.9,
        align="edge",
        color="#98df8a",
    )
    ax.set_title(f"Request arrival rate (bin={bin_width:.1f}s)")
    ax.set_xlabel("Time since first arrival (s)")
    ax.set_ylabel("Requests/s")
    ax.grid(True, linestyle="--", alpha=0.3)
    peak_rate = rates.max() if len(rates) else 0.0
    ax.text(
        0.98,
        0.85,
        f"peak {peak_rate:.1f} req/s",
        transform=ax.transAxes,
        ha="right",
        va="top",
        fontsize="small",
        color="#444444",
    )


def compute_sla_coverage(
    ids: Sequence[int],
    latencies: Sequence[float],
    thresholds: Sequence[float] = SLA_THRESHOLDS_MS,
) -> dict[float, Tuple[List[int], List[float]]]:
    result = {thr: ([], []) for thr in thresholds}
    if not ids or not latencies:
        return result
    sorted_pairs = sorted(zip(ids, latencies), key=lambda pair: pair[0])
    total = 0
    passed = {thr: 0 for thr in thresholds}
    for batch_id, latency in sorted_pairs:
        total += 1
        for thr in thresholds:
            if latency <= thr:
                passed[thr] += 1
            coverage = (passed[thr] / total) * 100.0
            result[thr][0].append(batch_id)
            result[thr][1].append(coverage)
    return result


def plot_sla_coverage(ax, ids: Sequence[int], latencies: Sequence[float]) -> None:
    series = compute_sla_coverage(ids, latencies)
    title = "SLA coverage (cumulative % <= threshold)"
    has_data = any(series[thr][0] for thr in SLA_THRESHOLDS_MS)
    if not has_data:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Coverage (%)")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    colors = ["#1f77b4", "#ff7f0e", "#2ca02c"]
    for idx, thr in enumerate(SLA_THRESHOLDS_MS):
        ids_series, coverage = series[thr]
        label = f"<= {thr:.0f} ms"
        ax.plot(ids_series, coverage, label=label, color=colors[idx % len(colors)])
    ax.set_title(title)
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Coverage (%)")
    ax.set_ylim(0, 105)
    ax.grid(True, linestyle="--", alpha=0.3)
    ax.legend(loc="lower right", fontsize="small")


def compute_empirical_cdf(values: Sequence[float]) -> Tuple[List[float], List[float]]:
    if not values:
        return [], []
    arr = np.sort(np.array(values, dtype=float))
    probabilities = np.arange(1, len(arr) + 1, dtype=float) / len(arr)
    return arr.tolist(), probabilities.tolist()


def plot_latency_cdf(
    ax,
    datasets: Sequence[Sequence[float]],
    labels: Sequence[str],
    title: str,
) -> None:
    colors = [
        "#1f77b4",
        "#ff7f0e",
        "#2ca02c",
        "#d62728",
        "#9467bd",
        "#17becf",
    ]
    plotted = False
    for idx, (series, label) in enumerate(zip(datasets, labels)):
        if not series:
            continue
        xs, ys = compute_empirical_cdf(series)
        ax.step(
            xs,
            ys,
            where="post",
            label=label,
            color=colors[idx % len(colors)],
        )
        plotted = True
    if not plotted:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel("Latency (ms)")
        ax.set_ylabel("Cumulative probability")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    ax.set_title(title)
    ax.set_xlabel("Latency (ms)")
    ax.set_ylabel("Cumulative probability")
    ax.set_ylim(0, 1.01)
    ax.grid(True, linestyle="--", alpha=0.3)
    ax.legend(loc="lower right", fontsize="small")


def plot_worker_cdf_grid(
    ax,
    worker_ids: Sequence[int],
    latencies: Sequence[float],
    max_workers: int = MAX_WORKER_CDFS,
) -> None:
    buckets: dict[int, List[float]] = {}
    for worker, latency in zip(worker_ids, latencies):
        if worker < 0:
            continue
        buckets.setdefault(worker, []).append(latency)
    if not buckets:
        ax.set_title("Worker latency CDFs (no data)")
        ax.set_axis_off()
        return
    top_workers = sorted(buckets.items(), key=lambda item: len(item[1]), reverse=True)[
        :max_workers
    ]
    cols = int(np.ceil(np.sqrt(len(top_workers))))
    rows = int(np.ceil(len(top_workers) / cols))
    fig = ax.figure
    subspec = ax.get_subplotspec()
    ax.remove()
    grid = GridSpecFromSubplotSpec(
        rows,
        cols,
        subplot_spec=subspec,
        wspace=0.25,
        hspace=0.25,
    )
    bbox = subspec.get_position(fig)
    for idx, (worker, samples) in enumerate(top_workers):
        row = idx // cols
        col = idx % cols
        cell = fig.add_subplot(grid[row, col])
        xs, ys = compute_empirical_cdf(samples)
        cell.step(xs, ys, where="post", color="#1f77b4")
        cell.set_title(f"worker {worker}")
        if row == rows - 1:
            cell.set_xlabel("Latency (ms)")
        else:
            cell.set_xticklabels([])
        if col == 0:
            cell.set_ylabel("CDF")
        else:
            cell.set_yticklabels([])
        cell.set_ylim(0, 1.01)
        cell.grid(True, linestyle="--", alpha=0.2)
    remaining = len(buckets) - len(top_workers)
    if remaining > 0:
        fig.text(
            bbox.x0 + bbox.width,
            bbox.y0 + bbox.height,
            f"+{remaining} more",
            fontsize="small",
            ha="right",
            va="bottom",
        )


def plot_rolling_percentiles(
    ax,
    batch_ids: Sequence[int],
    latencies: Sequence[float],
    worker_label: str,
    *,
    window_size: int = ROLLING_WINDOW,
) -> None:
    percentiles = (50, 95, 99)
    series = compute_rolling_percentiles(
        batch_ids,
        latencies,
        window_size=window_size,
        percentiles=percentiles,
    )
    has_data = any(series[perc][0] for perc in percentiles)
    title = f"{worker_label} rolling latency percentiles (window={window_size})"
    if not has_data:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Latency (ms)")
        ax.grid(True, linestyle="--", alpha=0.3)
        return
    colors = {50: "#1f77b4", 95: "#ff7f0e", 99: "#d62728"}
    for perc in percentiles:
        ids_series, values_series = series[perc]
        ax.plot(ids_series, values_series, label=f"P{perc}", color=colors.get(perc))
    ax.set_title(title)
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Latency (ms)")
    ax.grid(True, linestyle="--", alpha=0.3)
    ax.legend(loc="upper right", fontsize="small")


def plot_worker_task_distribution(
    ax,
    worker_ids: Sequence[int],
    jobs: Sequence[int],
) -> None:
    totals: dict[int, int] = {}
    for worker, job_count in zip(worker_ids, jobs):
        if worker < 0:
            continue
        totals[worker] = totals.get(worker, 0) + int(job_count)
    if not totals:
        ax.set_title("Worker task distribution (no data)")
        ax.set_xlabel("Worker ID")
        ax.set_ylabel("Total logical jobs")
        ax.grid(True, axis="y", linestyle="--", alpha=0.3)
        return
    sorted_items = sorted(totals.items(), key=lambda item: item[1], reverse=True)
    workers = [str(worker) for worker, _ in sorted_items]
    values = [item[1] for item in sorted_items]
    bars = ax.bar(workers, values, color="#17becf", edgecolor="black")
    ax.set_title("Worker task distribution")
    ax.set_xlabel("Worker ID")
    ax.set_ylabel("Total logical jobs")
    ax.grid(True, axis="y", linestyle="--", alpha=0.3)
    if bars:
        ax.bar_label(
            bars, labels=[str(v) for v in values], padding=2, fontsize="x-small"
        )


def main() -> int:
    args = parse_args()
    csv_path = args.summary_csv
    if not csv_path.exists():
        print(f"error: CSV not found: {csv_path}", file=sys.stderr)
        return 1

    try:
        data = load_latencies(csv_path)
    except ValueError as exc:
        print(f"error: {exc}", file=sys.stderr)
        return 1

    (
        all_ids,
        all_lat,
        all_workers,
        all_sizes,
        all_jobs,
        all_breakdowns,
        all_arrivals,
    ) = filter_latencies(data)
    (
        cpu_ids,
        cpu_lat,
        cpu_workers,
        cpu_sizes,
        cpu_jobs,
        cpu_breakdowns,
        cpu_arrivals,
    ) = filter_latencies(data, worker_type="cpu")
    (
        gpu_ids,
        gpu_lat,
        gpu_workers,
        gpu_sizes,
        gpu_jobs,
        gpu_breakdowns,
        gpu_arrivals,
    ) = filter_latencies(data, worker_type="cuda")
    worker_type_by_id = {batch_id: worker_type for batch_id, _, worker_type, *_ in data}
    all_worker_types = [
        worker_type_by_id.get(batch_id, "unknown") for batch_id in all_ids
    ]
    cpu_color = "#d62728"
    gpu_color = "#1f77b4"

    fig, axes_array = plt.subplots(35, 1, figsize=(12, 136), sharex=False)
    axes = list(axes_array)

    scatter_with_size(
        axes[0],
        all_ids,
        all_lat,
        all_sizes,
        "Latency vs batch size (multidim)",
        worker_types=all_worker_types,
    )

    has_worker_data = False
    if gpu_ids:
        axes[1].scatter(
            gpu_ids,
            gpu_lat,
            s=14,
            alpha=0.7,
            c=gpu_color,
            label="GPU",
        )
        has_worker_data = True
    if cpu_ids:
        axes[1].scatter(
            cpu_ids,
            cpu_lat,
            s=14,
            alpha=0.7,
            c=cpu_color,
            label="CPU",
        )
        has_worker_data = True
    if not has_worker_data and all_ids:
        axes[1].scatter(
            all_ids,
            all_lat,
            s=14,
            alpha=0.7,
            c="#7f7f7f",
            label="All workers",
        )
        has_worker_data = True
    axes[1].set_title(
        "All workers (CPU + GPU)"
        if has_worker_data
        else "All workers (CPU + GPU) (no data)"
    )
    axes[1].set_xlabel("Batch ID")
    axes[1].set_ylabel("Latency (ms)")
    axes[1].grid(True, linestyle="--", alpha=0.4)
    if has_worker_data:
        axes[1].legend(loc="upper right", fontsize="small")
    all_xlim = axes[1].get_xlim()
    all_ylim = axes[1].get_ylim()
    if all_ids:
        axes[0].set_xlim(all_xlim)
        axes[0].set_ylim(all_ylim)
    scatter_plot(axes[2], cpu_ids, cpu_lat, "CPU workers only", color=cpu_color)
    if all_ids:
        axes[2].set_xlim(all_xlim)
        axes[2].set_ylim(all_ylim)
    scatter_plot(axes[3], gpu_ids, gpu_lat, "GPU workers only")
    cum_ids, cum_vals = compute_cumulative_latency(all_ids, all_lat)
    if cum_ids:
        axes[4].plot(cum_ids, cum_vals, color="teal")
        axes[4].set_title("Cumulative latency vs batch ID")
        axes[4].set_xlabel("Batch ID")
        axes[4].set_ylabel("Cumulative latency (ms)")
        axes[4].grid(True, linestyle="--", alpha=0.3)
    else:
        axes[4].set_title("Cumulative latency vs batch ID (no data)")
        axes[4].grid(True, linestyle="--", alpha=0.3)

    throughput_ids, throughput_vals = compute_throughput(
        all_ids, all_jobs, window_size=THROUGHPUT_WINDOW
    )
    if throughput_ids:
        axes[5].plot(
            throughput_ids,
            throughput_vals,
            color="#9467bd",
            label=f"window={THROUGHPUT_WINDOW} batches",
        )
        axes[5].set_title("Throughput vs batch ID (requests per window)")
        axes[5].set_xlabel("Batch ID")
        axes[5].set_ylabel("Average logical jobs")
        axes[5].grid(True, linestyle="--", alpha=0.3)
        axes[5].legend(loc="lower right", fontsize="small")
    else:
        axes[5].set_title("Throughput vs batch ID (no data)")
        axes[5].grid(True, linestyle="--", alpha=0.3)

    plot_sla_coverage(axes[6], all_ids, all_lat)

    avg_ids, avg_vals = compute_moving_average(all_ids, all_lat, window=ROLLING_WINDOW)
    if avg_ids:
        axes[7].plot(avg_ids, avg_vals, color="purple")
        axes[7].set_title(f"Rolling average latency (window={ROLLING_WINDOW})")
        axes[7].set_xlabel("Batch ID")
        axes[7].set_ylabel("Latency (ms)")
        axes[7].grid(True, linestyle="--", alpha=0.3)
    else:
        axes[7].set_title("Rolling average latency (no data)")
        axes[7].grid(True, linestyle="--", alpha=0.3)

    plot_rolling_percentiles(
        axes[8], cpu_ids, cpu_lat, "CPU", window_size=ROLLING_WINDOW
    )
    plot_rolling_percentiles(
        axes[9], gpu_ids, gpu_lat, "GPU", window_size=ROLLING_WINDOW
    )

    plot_latency_stack(axes[10], all_ids, all_breakdowns)

    plot_phase_heatmap(axes[11], all_sizes, all_breakdowns)
    plot_phase_heatmap(axes[12], gpu_sizes, gpu_breakdowns)
    gpu_title = "Phase heatmap (avg ms) - GPU"
    if not gpu_sizes or not gpu_breakdowns:
        axes[12].set_title(f"{gpu_title} (no data)")
    else:
        axes[12].set_title(gpu_title)
    plot_phase_heatmap(axes[13], cpu_sizes, cpu_breakdowns)
    cpu_title = "Phase heatmap (avg ms) - CPU"
    if not cpu_sizes or not cpu_breakdowns:
        axes[13].set_title(f"{cpu_title} (no data)")
    else:
        axes[13].set_title(cpu_title)

    plot_worker_phase_heatmap(axes[14], all_workers, all_breakdowns)

    plot_phase_correlation(axes[15], all_breakdowns)
    plot_phase_correlation(
        axes[16],
        gpu_breakdowns,
        title="Phase correlation heatmap - GPU",
    )
    plot_phase_correlation(
        axes[17],
        cpu_breakdowns,
        title="Phase correlation heatmap - CPU",
    )

    plot_phase_waterfall(axes[18], cpu_breakdowns, gpu_breakdowns)
    plot_phase_pareto(axes[19], all_breakdowns)

    corr_xlim = None
    corr_ylim = None
    if all_sizes and all_lat:
        axes[20].scatter(all_sizes, all_lat, alpha=0.6, color="#17becf")
        if len(all_sizes) >= 2:
            sorted_pairs = sorted(zip(all_sizes, all_lat))
            xs = np.array([p[0] for p in sorted_pairs])
            ys = np.array([p[1] for p in sorted_pairs])
            coeffs = np.polyfit(xs, ys, deg=1)
            fit_x = np.linspace(xs.min(), xs.max(), num=200)
            fit_y = np.polyval(coeffs, fit_x)
            axes[20].plot(fit_x, fit_y, color="black", linestyle="--")
        axes[20].set_title("Latency vs batch size (correlation)")
        corr_xlim = axes[20].get_xlim()
        corr_ylim = axes[20].get_ylim()
    else:
        axes[20].set_title("Latency vs batch size (correlation) (no data)")
    axes[20].set_xlabel("Batch size")
    axes[20].set_ylabel("Latency (ms)")
    axes[20].grid(True, linestyle="--", alpha=0.3)

    if gpu_sizes and gpu_lat:
        axes[21].scatter(gpu_sizes, gpu_lat, alpha=0.6, color=gpu_color)
        if len(gpu_sizes) >= 2:
            sorted_pairs = sorted(zip(gpu_sizes, gpu_lat))
            xs = np.array([p[0] for p in sorted_pairs])
            ys = np.array([p[1] for p in sorted_pairs])
            coeffs = np.polyfit(xs, ys, deg=1)
            fit_x = np.linspace(xs.min(), xs.max(), num=200)
            fit_y = np.polyval(coeffs, fit_x)
            axes[21].plot(fit_x, fit_y, color="black", linestyle="--")
        axes[21].set_title("Latency vs batch size (correlation) - GPU")
    else:
        axes[21].set_title("Latency vs batch size (correlation) - GPU (no data)")
    axes[21].set_xlabel("Batch size")
    axes[21].set_ylabel("Latency (ms)")
    axes[21].grid(True, linestyle="--", alpha=0.3)
    if corr_xlim and corr_ylim:
        axes[21].set_xlim(corr_xlim)
        axes[21].set_ylim(corr_ylim)

    if cpu_sizes and cpu_lat:
        axes[22].scatter(cpu_sizes, cpu_lat, alpha=0.6, color=cpu_color)
        if len(cpu_sizes) >= 2:
            sorted_pairs = sorted(zip(cpu_sizes, cpu_lat))
            xs = np.array([p[0] for p in sorted_pairs])
            ys = np.array([p[1] for p in sorted_pairs])
            coeffs = np.polyfit(xs, ys, deg=1)
            fit_x = np.linspace(xs.min(), xs.max(), num=200)
            fit_y = np.polyval(coeffs, fit_x)
            axes[22].plot(fit_x, fit_y, color="black", linestyle="--")
        axes[22].set_title("Latency vs batch size (correlation) - CPU")
    else:
        axes[22].set_title("Latency vs batch size (correlation) - CPU (no data)")
    axes[22].set_xlabel("Batch size")
    axes[22].set_ylabel("Latency (ms)")
    axes[22].grid(True, linestyle="--", alpha=0.3)
    if corr_xlim and corr_ylim:
        axes[22].set_xlim(corr_xlim)
        axes[22].set_ylim(corr_ylim)

    size_counts_all = Counter(all_sizes)
    size_counts_cpu = Counter(cpu_sizes)
    size_counts_gpu = Counter(gpu_sizes)
    unique_sizes = sorted(
        set(size_counts_all.keys())
        | set(size_counts_cpu.keys())
        | set(size_counts_gpu.keys())
    )
    if not unique_sizes:
        axes[23].set_title("Batch size distribution (no data)")
        axes[23].set_xlabel("Batch size")
        axes[23].set_ylabel("Count")
        axes[23].grid(True, linestyle="--", alpha=0.3)
    else:
        series = []
        if size_counts_all:
            series.append(("All", size_counts_all, "gray", 0.7))
        if size_counts_cpu:
            series.append(("CPU", size_counts_cpu, cpu_color, 0.6))
        if size_counts_gpu:
            series.append(("GPU", size_counts_gpu, "blue", 0.6))
        bar_groups = max(1, len(series))
        width = 0.8 / bar_groups
        positions = np.arange(len(unique_sizes))
        handles = []
        for idx, (label, counts, color, alpha) in enumerate(series):
            offset = (idx - (bar_groups - 1) / 2) * width
            heights = [counts.get(size, 0) for size in unique_sizes]
            bars = axes[23].bar(
                positions + offset,
                heights,
                width=width * 0.9,
                label=label,
                color=color,
                alpha=alpha,
                edgecolor="black",
            )
            handles.append(bars)
        axes[23].set_xticks(positions, [str(size) for size in unique_sizes])
        axes[23].set_title("Batch size distribution")
        axes[23].set_xlabel("Batch size")
        axes[23].set_ylabel("Count")
        if handles:
            axes[23].legend()
        axes[23].grid(True, linestyle="--", alpha=0.3)

    violin_data = []
    labels = []
    if cpu_lat:
        violin_data.append(cpu_lat)
        labels.append("CPU")
    if gpu_lat:
        violin_data.append(gpu_lat)
        labels.append("GPU")
    if violin_data:
        axes[24].violinplot(violin_data, showmeans=True, showmedians=False)
        axes[24].set_xticks(range(1, len(labels) + 1), labels)
        axes[24].set_title("Latency distribution (violin plot)")
        axes[24].set_ylabel("Latency (ms)")
        axes[24].grid(True, linestyle="--", alpha=0.3)
    else:
        axes[24].set_title("Latency distribution (no data)")
        axes[24].grid(True, linestyle="--", alpha=0.3)

    worker_cdf_data: List[Sequence[float]] = []
    worker_cdf_labels: List[str] = []
    if cpu_lat:
        worker_cdf_data.append(cpu_lat)
        worker_cdf_labels.append("CPU")
    if gpu_lat:
        worker_cdf_data.append(gpu_lat)
        worker_cdf_labels.append("GPU")
    plot_latency_cdf(
        axes[25],
        worker_cdf_data,
        worker_cdf_labels,
        "Latency CDF by worker type",
    )

    plot_worker_cdf_grid(axes[26], all_workers, all_lat)

    size_latencies: dict[int, List[float]] = {}
    for size, latency in zip(all_sizes, all_lat):
        size_latencies.setdefault(size, []).append(latency)
    top_sizes = sorted(
        size_latencies.items(), key=lambda item: len(item[1]), reverse=True
    )[:MAX_BATCH_CDF_SERIES]
    batch_cdf_data = [item[1] for item in top_sizes]
    batch_cdf_labels = [f"size {item[0]}" for item in top_sizes]
    plot_latency_cdf(
        axes[27],
        batch_cdf_data,
        batch_cdf_labels,
        "Latency CDF by batch size",
    )

    plot_worker_phase_utilization(axes[28], all_workers, all_breakdowns)
    plot_worker_boxplots(axes[29], all_workers, all_lat)
    plot_worker_task_distribution(axes[30], all_workers, all_jobs)
    plot_worker_time_heatmap(axes[31], all_workers, all_ids, all_lat)
    plot_request_arrival_timeline(axes[32], all_arrivals)
    plot_request_arrival_rate(axes[33], all_arrivals)
    axes[34].remove()
    axes[34] = fig.add_subplot(35, 1, 35, projection="polar")
    plot_worker_radar(axes[34], all_workers, all_breakdowns)
    fig.subplots_adjust(hspace=0.6, top=0.98, bottom=0.02)

    if args.output:
        output_path = args.output
        output_path.parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(output_path, dpi=150)
        print(f"Saved plots to {output_path}")
    else:
        plt.show()
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
