#!/usr/bin/env python3
"""
Plot batch latencies from batching_trace_summary.csv.

This script expects the CSV generated by batching tracing (when trace_enabled is
true). It produces three scatter plots:

1. Combined batches (CPU + GPU)
2. CPU-only batches
3. GPU-only batches

Usage:
  ./scripts/plot_batch_summary.py PATH_TO/batching_trace_summary.csv
  ./scripts/plot_batch_summary.py PATH_TO/file.csv --output plots.png
"""

from __future__ import annotations

import argparse
import csv
import sys
from collections import Counter, deque
from itertools import cycle
from pathlib import Path
from typing import Iterable, List, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np

PHASE_LABELS = [
    "queue",
    "batch",
    "submit",
    "scheduling",
    "codelet",
    "inference",
    "callback",
]

PHASE_COLORS = {
    "queue": "#1f77b4",
    "batch": "#ff7f0e",
    "submit": "#2ca02c",
    "scheduling": "#d62728",
    "codelet": "#9467bd",
    "inference": "#8c564b",
    "callback": "#e377c2",
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Plot batch latencies from batching_trace_summary.csv."
    )
    parser.add_argument(
        "summary_csv",
        type=Path,
        help="Path to batching_trace_summary.csv",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        help=(
            "Optional output image path (PNG/JPG/etc.). "
            "If omitted the plots are shown interactively."
        ),
    )
    return parser.parse_args()


def load_latencies(
    csv_path: Path,
) -> List[Tuple[int, float, str, int, int, int, Tuple[float, ...]]]:
    batches: List[Tuple[int, float, str, int, int, int, Tuple[float, ...]]] = []
    with csv_path.open(newline="") as handle:
        reader = csv.DictReader(handle)
        required = {
            "batch_id",
            "total_ms",
            "worker_type",
            "worker_id",
            "batch_size",
        }
        required.update(f"{label}_ms" for label in PHASE_LABELS)
        missing = sorted(required - set(reader.fieldnames or []))
        if missing:
            raise ValueError(
                f"{csv_path} is missing required columns: {', '.join(missing)}"
            )
        for row in reader:
            try:
                batch_id = int(row["batch_id"])
                latency = float(row["total_ms"])
            except ValueError as exc:
                raise ValueError(f"Invalid numeric values in row: {row}") from exc
            worker_type = (row.get("worker_type") or "unknown").strip().lower()
            worker_id = int(row["worker_id"])
            batch_size = int(row["batch_size"])
            logical_jobs_field = row.get("logical_jobs")
            logical_jobs = (
                int(logical_jobs_field)
                if logical_jobs_field not in (None, "")
                else batch_size
            )
            breakdown = tuple(float(row[f"{label}_ms"]) for label in PHASE_LABELS)
            batches.append(
                (
                    batch_id,
                    latency,
                    worker_type,
                    worker_id,
                    batch_size,
                    logical_jobs,
                    breakdown,
                )
            )
    return batches


def filter_latencies(
    data: Iterable[Tuple[int, float, str, int, int, int, Tuple[float, ...]]],
    *,
    worker_type: str | None = None,
) -> Tuple[
    List[int], List[float], List[int], List[int], List[int], List[Tuple[float, ...]]
]:
    ids: List[int] = []
    latencies: List[float] = []
    worker_ids: List[int] = []
    batch_sizes: List[int] = []
    logical_jobs: List[int] = []
    breakdowns: List[Tuple[float, ...]] = []
    for (
        batch_id,
        latency,
        device,
        worker_id,
        batch_size,
        logical_job_count,
        breakdown,
    ) in data:
        if worker_type is None or device == worker_type:
            ids.append(batch_id)
            latencies.append(latency)
            worker_ids.append(worker_id)
            batch_sizes.append(batch_size)
            logical_jobs.append(logical_job_count)
            breakdowns.append(breakdown)
    return ids, latencies, worker_ids, batch_sizes, logical_jobs, breakdowns


def plot_latency_stack(
    ax,
    batch_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    if not batch_ids or not breakdowns:
        ax.set_title("Latency composition per batch (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Latency contribution (ms)")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    labels = PHASE_LABELS
    components = list(map(list, zip(*breakdowns)))
    bottom = [0.0] * len(batch_ids)
    ax.set_title("Latency composition per batch")
    for values, label in zip(components, labels):
        ax.bar(batch_ids, values, bottom=bottom, label=label, width=0.6)
        bottom = [b + v for b, v in zip(bottom, values)]
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Latency contribution (ms)")
    ax.legend(loc="upper right", fontsize="small")
    ax.grid(True, linestyle="--", alpha=0.3)


def scatter_plot(
    ax, x: List[int], y: List[float], title: str, *, color: str | None = None
) -> None:
    if not x:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Latency (ms)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    ax.scatter(x, y, s=14, alpha=0.7, c=color)
    ax.set_title(title)
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Latency (ms)")
    ax.grid(True, linestyle="--", alpha=0.4)


def plot_violin(
    ax,
    data: Sequence[float],
    label: str,
    color: str | None = None,
) -> None:
    if not data:
        ax.set_title(f"{label} (no data)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    parts = ax.violinplot(data, showmeans=True, showmedians=False)
    if color:
        for pc in parts["bodies"]:
            pc.set_facecolor(color)
            pc.set_edgecolor("black")
        parts["cbars"].set_edgecolor(color)
        parts["cmins"].set_edgecolor(color)
        parts["cmaxes"].set_edgecolor(color)
    ax.set_title(label)
    ax.grid(True, linestyle="--", alpha=0.4)


def scatter_with_size(
    ax,
    x: Sequence[int],
    y: Sequence[float],
    sizes: Sequence[int],
    title: str,
) -> None:
    if not x or not y or not sizes:
        ax.set_title(f"{title} (no data)")
        ax.set_xlabel("Batch ID")
        ax.set_ylabel("Latency (ms)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    max_size = max(sizes)
    min_size = max(1, min(sizes))
    scale = []
    for value in sizes:
        norm = value / max_size if max_size > 0 else 0.0
        scale.append(50 + norm * 200)
    scatter = ax.scatter(
        x,
        y,
        s=scale,
        c=sizes,
        cmap="viridis",
        alpha=0.7,
    )
    ax.set_title(title)
    ax.set_xlabel("Batch ID")
    ax.set_ylabel("Latency (ms)")
    ax.grid(True, linestyle="--", alpha=0.4)
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label("Batch size")


def plot_worker_boxplots(
    ax,
    worker_ids: Sequence[int],
    latencies: Sequence[float],
) -> None:
    data = {}
    for worker, latency in zip(worker_ids, latencies):
        if worker < 0:
            continue
        data.setdefault(worker, []).append(latency)
    if not data:
        ax.set_title("Worker latency boxplots (no data)")
        ax.set_xlabel("Worker ID")
        ax.set_ylabel("Latency (ms)")
        ax.grid(True, linestyle="--", alpha=0.4)
        return
    sorted_items = sorted(data.items(), key=lambda item: item[0])
    ax.boxplot(
        [item[1] for item in sorted_items],
        labels=[item[0] for item in sorted_items],
        showmeans=True,
    )
    ax.set_title("Worker latency boxplots")
    ax.set_xlabel("Worker ID")
    ax.set_ylabel("Latency (ms)")
    ax.grid(True, linestyle="--", alpha=0.4)


def plot_worker_radar(
    ax,
    worker_ids: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_workers: int = 4,
) -> None:
    contributions: dict[int, np.ndarray] = {}
    counts: dict[int, int] = {}
    for worker, phases in zip(worker_ids, breakdowns):
        if worker < 0:
            continue
        contributions.setdefault(worker, np.zeros(len(PHASE_LABELS)))
        contributions[worker] += np.array(phases)
        counts[worker] = counts.get(worker, 0) + 1

    if not contributions:
        ax.set_title("Worker radar (no data)")
        ax.set_axis_off()
        return

    averages = []
    for worker, total in contributions.items():
        average = total / counts[worker]
        averages.append((worker, average))
    averages.sort(key=lambda item: counts[item[0]], reverse=True)
    averages = averages[:max_workers]

    angles = np.linspace(0, 2 * np.pi, len(PHASE_LABELS), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))
    color_cycle = cycle(["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728"])
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_xticks(angles[:-1], PHASE_LABELS)
    ax.set_title("Worker radar (avg phase ms)")

    for worker, avg in averages:
        values = np.concatenate((avg, [avg[0]]))
        color = next(color_cycle)
        ax.plot(angles, values, label=f"worker {worker}", color=color)
        ax.fill(angles, values, alpha=0.1, color=color)

    ax.legend(loc="upper right", bbox_to_anchor=(1.2, 1.1), fontsize="small")


def plot_worker_time_heatmap(
    ax,
    worker_ids: Sequence[int],
    batch_ids: Sequence[int],
    latencies: Sequence[float],
    max_workers: int = 12,
    bucket_size: int = 100,
) -> None:
    if not worker_ids or not batch_ids or not latencies:
        ax.set_title("Worker-time heatmap (no data)")
        ax.set_xlabel("Batch bucket")
        ax.set_ylabel("Worker ID")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    sorted_workers = sorted(set(worker_ids), key=lambda wid: (wid < 0, wid))[
        :max_workers
    ]
    worker_index = {wid: idx for idx, wid in enumerate(sorted_workers)}
    bucket_map: dict[tuple[int, int], list[float]] = {}
    for wid, bid, latency in zip(worker_ids, batch_ids, latencies):
        if wid not in worker_index or wid < 0:
            continue
        bucket = bid // bucket_size
        bucket_map.setdefault((wid, bucket), []).append(latency)

    if not bucket_map:
        ax.set_title("Worker-time heatmap (no data)")
        ax.set_xlabel("Batch bucket")
        ax.set_ylabel("Worker ID")
        ax.grid(True, linestyle="--", alpha=0.3)
        return

    buckets = sorted({bucket for _, bucket in bucket_map.keys()})
    heatmap = np.full((len(sorted_workers), len(buckets)), np.nan)
    bucket_index = {bucket: idx for idx, bucket in enumerate(buckets)}
    for (wid, bucket), samples in bucket_map.items():
        row = worker_index[wid]
        col = bucket_index[bucket]
        heatmap[row, col] = np.mean(samples)

    im = ax.imshow(heatmap, aspect="auto", cmap="plasma", origin="lower")
    ax.set_xticks(
        range(len(buckets)), [str(bucket * bucket_size) for bucket in buckets]
    )
    ax.set_yticks(range(len(sorted_workers)), [str(w) for w in sorted_workers])
    ax.set_xlabel("Batch ID bucket")
    ax.set_ylabel("Worker ID")
    ax.set_title("Worker-time heatmap (avg latency)")
    plt.colorbar(im, ax=ax, label="Latency (ms)")


def plot_phase_heatmap(
    ax,
    batch_sizes: Sequence[int],
    breakdowns: Sequence[Tuple[float, ...]],
    max_columns: int = 12,
) -> None:
    if not batch_sizes or not breakdowns:
        ax.set_title("Phase heatmap (no data)")
        ax.set_xlabel("Batch size")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return

    counter = Counter(batch_sizes)
    top_sizes = [size for size, _ in counter.most_common(max_columns)]
    top_sizes.sort()
    if not top_sizes:
        ax.set_title("Phase heatmap (no data)")
        ax.set_xlabel("Batch size")
        ax.set_ylabel("Phase")
        ax.grid(True, linestyle="--", alpha=0.4)
        return

    index = {size: idx for idx, size in enumerate(top_sizes)}
    totals = np.zeros((len(PHASE_LABELS), len(top_sizes)), dtype=float)
    counts = np.zeros_like(totals)

    for size, phases in zip(batch_sizes, breakdowns):
        col = index.get(size)
        if col is None:
            continue
        for row, value in enumerate(phases):
            totals[row, col] += value
            counts[row, col] += 1

    with np.errstate(invalid="ignore"):
        averages = np.divide(totals, counts, where=counts > 0)

    masked = np.ma.masked_invalid(averages)
    im = ax.imshow(masked, aspect="auto", cmap="magma", origin="lower")
    ax.set_xticks(range(len(top_sizes)), [str(size) for size in top_sizes])
    ax.set_yticks(range(len(PHASE_LABELS)), PHASE_LABELS)
    ax.set_xlabel("Batch size")
    ax.set_ylabel("Phase")
    ax.set_title("Phase heatmap (avg ms)")
    plt.colorbar(im, ax=ax, label="Average duration (ms)")


def plot_phase_pareto(
    ax,
    breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    if not breakdowns:
        ax.set_title("Phase Pareto (no data)")
        ax.set_xlabel("Phase")
        ax.set_ylabel("Average duration (ms)")
        ax.grid(True, axis="y", linestyle="--", alpha=0.3)
        return
    totals = np.zeros(len(PHASE_LABELS), dtype=float)
    for phases in breakdowns:
        totals += np.array(phases)
    averages = totals / len(breakdowns)
    sorted_indices = np.argsort(-averages)
    sorted_labels = [PHASE_LABELS[idx] for idx in sorted_indices]
    sorted_values = averages[sorted_indices]
    ax.bar(sorted_labels, sorted_values, color="#6a4c93")
    ax.set_title("Phase Pareto (avg ms)")
    ax.set_xlabel("Phase")
    ax.set_ylabel("Average duration (ms)")
    ax.grid(True, axis="y", linestyle="--", alpha=0.3)


def plot_phase_waterfall(
    ax,
    cpu_breakdowns: Sequence[Tuple[float, ...]],
    gpu_breakdowns: Sequence[Tuple[float, ...]],
) -> None:
    categories = []
    data = []
    if cpu_breakdowns:
        categories.append("CPU")
        data.append(np.array(cpu_breakdowns))
    if gpu_breakdowns:
        categories.append("GPU")
        data.append(np.array(gpu_breakdowns))
    if not categories:
        ax.set_title("Phase waterfall (no data)")
        ax.set_xlabel("Total time (ms)")
        ax.set_ylabel("Worker type")
        ax.grid(True, axis="x", linestyle="--", alpha=0.3)
        return

    y_positions = np.arange(len(categories))
    legend_added = set()
    for idx, (label, breakdown_array) in enumerate(zip(categories, data)):
        totals = breakdown_array.sum(axis=0)
        left = 0.0
        for phase, value in zip(PHASE_LABELS, totals):
            color = PHASE_COLORS.get(phase, None)
            bar = ax.barh(
                y_positions[idx],
                value,
                left=left,
                color=color,
                label=phase if phase not in legend_added else None,
            )
            left += value
            legend_added.add(phase)
    ax.set_yticks(y_positions, categories)
    ax.set_xlabel("Total time (ms)")
    ax.set_title("Phase waterfall (CPU vs GPU)")
    ax.grid(True, axis="x", linestyle="--", alpha=0.3)
    ax.legend(loc="upper right", fontsize="small")


def compute_moving_average(
    ids: Sequence[int], values: Sequence[float], window: int = 50
) -> Tuple[List[int], List[float]]:
    if not ids or not values:
        return [], []
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    sorted_ids = [pair[0] for pair in sorted_pairs]
    sorted_vals = [pair[1] for pair in sorted_pairs]
    window = max(1, min(window, len(sorted_vals)))
    rolling_ids: List[int] = []
    rolling_vals: List[float] = []
    window_sum = 0.0
    current = deque()
    for batch_id, value in zip(sorted_ids, sorted_vals):
        current.append(value)
        window_sum += value
        if len(current) > window:
            window_sum -= current.popleft()
        rolling_ids.append(batch_id)
        rolling_vals.append(window_sum / len(current))
    return rolling_ids, rolling_vals


def compute_cumulative_latency(
    ids: Sequence[int], values: Sequence[float]
) -> Tuple[List[int], List[float]]:
    if not ids or not values:
        return [], []
    sorted_pairs = sorted(zip(ids, values), key=lambda pair: pair[0])
    cum_sum = 0.0
    cum_ids: List[int] = []
    cum_vals: List[float] = []
    for batch_id, latency in sorted_pairs:
        cum_sum += latency
        cum_ids.append(batch_id)
        cum_vals.append(cum_sum)
    return cum_ids, cum_vals


def compute_throughput(
    ids: Sequence[int],
    jobs: Sequence[int],
    window_size: int = 100,
) -> Tuple[List[int], List[float]]:
    if not ids or not jobs:
        return [], []
    sorted_pairs = sorted(zip(ids, jobs), key=lambda pair: pair[0])
    throughput_ids: List[int] = []
    throughput_vals: List[float] = []
    window = deque()
    jobs_sum = 0
    batch_sum = 0
    for batch_id, job_count in sorted_pairs:
        window.append((batch_id, job_count))
        jobs_sum += job_count
        batch_sum += 1
        while window and batch_sum > window_size:
            _, oldest_jobs = window.popleft()
            jobs_sum -= oldest_jobs
            batch_sum -= 1
        throughput_ids.append(batch_id)
        throughput_vals.append(jobs_sum / max(1, batch_sum))
    return throughput_ids, throughput_vals


def main() -> int:
    args = parse_args()
    csv_path = args.summary_csv
    if not csv_path.exists():
        print(f"error: CSV not found: {csv_path}", file=sys.stderr)
        return 1

    try:
        data = load_latencies(csv_path)
    except ValueError as exc:
        print(f"error: {exc}", file=sys.stderr)
        return 1

    all_ids, all_lat, all_workers, all_sizes, all_jobs, all_breakdowns = (
        filter_latencies(data)
    )
    cpu_ids, cpu_lat, cpu_workers, cpu_sizes, cpu_jobs, cpu_breakdowns = (
        filter_latencies(data, worker_type="cpu")
    )
    gpu_ids, gpu_lat, gpu_workers, gpu_sizes, gpu_jobs, gpu_breakdowns = (
        filter_latencies(data, worker_type="cuda")
    )
    cpu_color = "#d62728"

    fig, axes_array = plt.subplots(17, 1, figsize=(12, 64), sharex=False)
    axes = list(axes_array)
    scatter_plot(axes[0], all_ids, all_lat, "All workers (CPU + GPU)")
    if cpu_ids:
        axes[0].scatter(cpu_ids, cpu_lat, s=14, alpha=0.7, c=cpu_color)
    scatter_with_size(
        axes[1], all_ids, all_lat, all_sizes, "Latency vs batch size (multidim)"
    )
    scatter_plot(axes[2], cpu_ids, cpu_lat, "CPU workers only", color=cpu_color)
    scatter_plot(axes[3], gpu_ids, gpu_lat, "GPU workers only")
    cum_ids, cum_vals = compute_cumulative_latency(all_ids, all_lat)
    if cum_ids:
        axes[4].plot(cum_ids, cum_vals, color="teal")
        axes[4].set_title("Cumulative latency vs batch ID")
        axes[4].set_xlabel("Batch ID")
        axes[4].set_ylabel("Cumulative latency (ms)")
        axes[4].grid(True, linestyle="--", alpha=0.3)
    else:
        axes[4].set_title("Cumulative latency vs batch ID (no data)")
        axes[4].grid(True, linestyle="--", alpha=0.3)

    throughput_ids, throughput_vals = compute_throughput(all_ids, all_jobs)
    if throughput_ids:
        axes[5].plot(throughput_ids, throughput_vals, color="#9467bd")
        axes[5].set_title("Throughput vs batch ID (requests per window)")
        axes[5].set_xlabel("Batch ID")
        axes[5].set_ylabel("Average logical jobs")
        axes[5].grid(True, linestyle="--", alpha=0.3)
    else:
        axes[5].set_title("Throughput vs batch ID (no data)")
        axes[5].grid(True, linestyle="--", alpha=0.3)

    avg_ids, avg_vals = compute_moving_average(all_ids, all_lat)
    if avg_ids:
        axes[6].plot(avg_ids, avg_vals, color="purple")
        axes[6].set_title("Rolling average latency (window=50)")
        axes[6].set_xlabel("Batch ID")
        axes[6].set_ylabel("Latency (ms)")
        axes[6].grid(True, linestyle="--", alpha=0.3)
    else:
        axes[6].set_title("Rolling average latency (no data)")
        axes[6].grid(True, linestyle="--", alpha=0.3)

    plot_latency_stack(axes[7], all_ids, all_breakdowns)
    plot_phase_heatmap(axes[8], all_sizes, all_breakdowns)
    plot_phase_waterfall(axes[9], cpu_breakdowns, gpu_breakdowns)
    plot_phase_pareto(axes[10], all_breakdowns)

    axes[11].scatter(all_sizes, all_lat, alpha=0.6, color="#17becf")
    if len(all_sizes) >= 2:
        sorted_pairs = sorted(zip(all_sizes, all_lat))
        xs = np.array([p[0] for p in sorted_pairs])
        ys = np.array([p[1] for p in sorted_pairs])
        coeffs = np.polyfit(xs, ys, deg=1)
        fit_x = np.linspace(xs.min(), xs.max(), num=200)
        fit_y = np.polyval(coeffs, fit_x)
        axes[11].plot(fit_x, fit_y, color="black", linestyle="--")
    axes[11].set_title("Latency vs batch size (correlation)")
    axes[11].set_xlabel("Batch size")
    axes[11].set_ylabel("Latency (ms)")
    axes[11].grid(True, linestyle="--", alpha=0.3)

    axes[12].hist(all_sizes, bins=30, alpha=0.7, label="All", color="gray")
    if cpu_sizes:
        axes[12].hist(cpu_sizes, bins=30, alpha=0.5, label="CPU", color=cpu_color)
    if gpu_sizes:
        axes[12].hist(gpu_sizes, bins=30, alpha=0.5, label="GPU", color="blue")
    axes[12].set_title("Batch size distribution")
    axes[12].set_xlabel("Batch size")
    axes[12].set_ylabel("Count")
    axes[12].legend()
    axes[12].grid(True, linestyle="--", alpha=0.3)

    violin_data = []
    labels = []
    if cpu_lat:
        violin_data.append(cpu_lat)
        labels.append("CPU")
    if gpu_lat:
        violin_data.append(gpu_lat)
        labels.append("GPU")
    if violin_data:
        axes[13].violinplot(violin_data, showmeans=True, showmedians=False)
        axes[13].set_xticks(range(1, len(labels) + 1), labels)
        axes[13].set_title("Latency distribution (violin plot)")
        axes[13].set_ylabel("Latency (ms)")
        axes[13].grid(True, linestyle="--", alpha=0.3)
    else:
        axes[13].set_title("Latency distribution (no data)")
        axes[13].grid(True, linestyle="--", alpha=0.3)

    plot_worker_boxplots(axes[14], all_workers, all_lat)
    plot_worker_time_heatmap(axes[15], all_workers, all_ids, all_lat)
    axes[16].remove()
    axes[16] = fig.add_subplot(17, 1, 17, projection="polar")
    plot_worker_radar(axes[16], all_workers, all_breakdowns)
    fig.tight_layout()

    if args.output:
        output_path = args.output
        output_path.parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(output_path, dpi=150)
        print(f"Saved plots to {output_path}")
    else:
        plt.show()
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
