#StarPU Inference Server Docs

This directory collects the documentation needed to install, run, and tune the StarPU Inference Server.

## Getting Started

- Start with `installation.md` for building the server directly on your machine.
- Prefer containerized setups? Follow `installation-docker.md` for Docker instructions.
- Once installed, `usage.md` walks through running inference workloads with the provided tooling.

## Configuration Reference

- Use `configuration.md` to review runtime options, environment variables, and parameters.
